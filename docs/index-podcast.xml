<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>An Introduction to Statistical Learning</title>
<link>https://orenbochman.github.io/notes-islr/#category=podcast</link>
<atom:link href="https://orenbochman.github.io/notes-islr/index-podcast.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<image>
<url>https://orenbochman.github.io/notes-islr/images/nlp-brain-wordcloud.jpg</url>
<title>An Introduction to Statistical Learning</title>
<link>https://orenbochman.github.io/notes-islr/#category=podcast</link>
</image>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Thu, 19 Sep 2024 21:00:00 GMT</lastBuildDate>
<item>
  <title>Chapter 13: Multiple Testing</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch13-multiple-testing/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-v1.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v1.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/LvySJGj-88U?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v1.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Opening Remarks
</figcaption>
</figure>
</div><div id="fig-v1.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v1.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/B9s8rpdNxU0?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v1.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples and Framework
</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Multiple Testing üî¨üî¨üî¨
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Multiple Testing in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Multiple Testing in a nutshell"></a></p>
<figcaption>Multiple Testing in a nutshell</figcaption>
</figure>
</div>
<p>Multiple testing refers to the practice of conducting multiple hypothesis tests simultaneously, which can lead to an increased risk of making at least one Type I error. The Family-wise Error Rate (FWER) and False Discovery Rate (FDR) are two common error rates used to control for the risks of multiple testing. The Bonferroni correction, Holm‚Äôs method, and the Benjamini-Hochberg procedure are popular methods to adjust p-values and control for these error rates. Re-sampling methods like permutation testing can also be used to approximate the null distribution of the test statistic when theoretical distributions are unknown or assumptions about the data are difficult to justify.</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<p>Glossary of Key Terms</p>
<dl>
<dt>Hypothesis Testing</dt>
<dd>
A statistical method used to assess the evidence provided by data against a specific null hypothesis in favor of an alternative hypothesis.
</dd>
<dt>Null Hypothesis (H0)</dt>
<dd>
The default statement of no effect or no difference that we aim to disprove using statistical evidence.
</dd>
<dt>Alternative Hypothesis (Ha)</dt>
<dd>
The statement that contradicts the null hypothesis and represents the claim we want to support if there is enough evidence.
</dd>
<dt>Test Statistic</dt>
<dd>
A numerical summary of the data that is used to assess the compatibility of the data with the null hypothesis.
</dd>
<dt>P-value</dt>
<dd>
The probability of observing a test statistic as extreme as or more extreme than the one calculated from the data, assuming the null hypothesis is true.
</dd>
<dt>Type I Error</dt>
<dd>
Rejecting a true null hypothesis (false positive).
</dd>
<dt>Type II Error</dt>
<dd>
Failing to reject a false null hypothesis (false negative).
</dd>
<dt>Multiple Hypothesis Testing</dt>
<dd>
Performing multiple hypothesis tests simultaneously, which increases the probability of making at least one Type I error.
</dd>
<dt>Family-wise Error Rate (FWER)</dt>
<dd>
The probability of making at least one Type I error among all hypotheses tested.
</dd>
<dt>False Discovery Rate (FDR)</dt>
<dd>
The expected proportion of falsely rejected null hypotheses among all rejected null hypotheses.
</dd>
<dt>Bonferroni Correction</dt>
<dd>
A method to control the FWER by dividing the desired overall significance level by the number of tests.
</dd>
<dt>Holm‚Äôs Method</dt>
<dd>
A step-down method to control the FWER that adjusts p-value thresholds sequentially, potentially leading to more rejections than the Bonferroni correction.
</dd>
<dt>Benjamini-Hochberg Procedure</dt>
<dd>
A method to control the FDR by identifying the largest p-value that satisfies a specific criterion and rejecting all null hypotheses with p-values less than or equal to this identified p-value.
</dd>
<dt>Re-sampling Methods</dt>
<dd>
Techniques like permutation testing that use random sampling with replacement from the observed data to approximate the null distribution of the test statistic, especially helpful when theoretical null distributions are unknown or assumptions about the data are difficult to justify.
</dd>
<dt>Permutation Testing</dt>
<dd>
A re-sampling method where data points are randomly reassigned to different groups to create new datasets under the assumption of the null hypothesis. This allows for the estimation of the null distribution of the test statistic without relying on specific distributional assumptions.
</dd>
</dl>
<section id="chapter-outline" class="level2">
<h2 class="anchored" data-anchor-id="chapter-outline">Chapter Outline</h2>
<p><strong>Central Problem</strong>: When testing multiple hypotheses simultaneously, the probability of making at least one Type I error (false positive) increases dramatically. This necessitates the use of specific methods to control this error rate.</p>
<ol type="1">
<li>Key Concepts:</li>
</ol>
<p>Type I Error: Rejecting the null hypothesis when it is actually true (false positive). Type II Error: Failing to reject the null hypothesis when it is actually false (false negative). Family-Wise Error Rate (FWER): The probability of making at least one Type I error across all tested hypotheses. False Discovery Rate (FDR): The expected proportion of rejected null hypotheses that are actually false positives. Multiple Testing Correction: Adjusting p-values or rejection thresholds to account for the increased risk of Type I errors when testing multiple hypotheses.</p>
<ol start="2" type="1">
<li>Methods for Controlling FWER:</li>
</ol>
<p>Bonferroni Correction: The simplest and most conservative method. It divides the desired alpha level (the probability of making a Type I error) by the number of hypotheses being tested (m). This new, stricter alpha level is then used as the threshold for rejecting each individual hypothesis. Formula: Reject H0j if p-value &lt; Œ±/m Advantages: Easy to implement. Disadvantages: Can be overly conservative, leading to higher Type II error rates, especially with a large number of hypotheses. Holm‚Äôs Method: A step-down procedure that offers more power than Bonferroni while still controlling the FWER. It orders the p-values from smallest to largest and compares each p-value to a sequentially adjusted alpha level. Formula: Reject H0j if p(j) &lt; Œ±/(m + 1 - j), where p(j) is the jth smallest p-value. Advantages: More powerful than Bonferroni. Disadvantages: Still relatively conservative. Tukey‚Äôs and Scheff√©‚Äôs Methods: Specialized methods tailored for specific types of multiple comparisons. Tukey‚Äôs method is designed for all pairwise comparisons of means, while Scheff√©‚Äôs method is more general and allows for any linear combination of means to be tested.</p>
<ol start="3" type="1">
<li>Methods for Controlling FDR:</li>
</ol>
<p>Benjamini-Hochberg Procedure: A step-up procedure that controls the FDR at a desired level (q). It orders the p-values from smallest to largest and compares each p-value to a sequentially adjusted q-value threshold. Formula: Reject H0j if p(j) &lt; qj/m Advantages: Less conservative than FWER control methods, leading to higher power. Disadvantages: May lead to a higher proportion of false positives compared to FWER control methods. 4. Re-Sampling Approaches:</p>
<p>When the theoretical null distribution of a test statistic is unknown, re-sampling methods (e.g., permutation tests) can be used to approximate the null distribution and calculate p-values. This is particularly useful for small sample sizes or complex test statistics.</p>
<ol start="5" type="1">
<li>Illustrations: There are many examples using simulated data and real datasets (Fund dataset, Khan dataset) to demonstrate the impact of multiple testing on error rates and the effectiveness of different correction methods.</li>
</ol>
</section>
<section id="key-quotes" class="level2">
<h2 class="anchored" data-anchor-id="key-quotes">Key Quotes:</h2>
<blockquote class="blockquote">
<p>‚ÄúThe Bonferroni correction gives us peace of mind that we have not falsely rejected too many null hypotheses, but for a price: we reject few null hypotheses, and thus will typically make quite a few Type II errors.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúIf m = 10,000, then we expect to falsely reject 100 null hypotheses by chance! That‚Äôs a lot of Type I errors, i.e., false positives!‚Äù</p>
</blockquote>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>Multiple hypothesis testing is a common challenge in data analysis, requiring careful consideration of error control methods. Choosing the appropriate method depends on the specific research question, desired level of error control, and available resources. Re-sampling approaches offer a flexible alternative when theoretical null distributions are unavailable.</p>
</section>
<section id="multiple-hypothesis-testing-study-guide" class="level2">
<h2 class="anchored" data-anchor-id="multiple-hypothesis-testing-study-guide">Multiple Hypothesis Testing Study Guide</h2>
<p>Short Answer Questions Instructions: Answer the following questions in 2-3 sentences each.</p>
<ol type="1">
<li>What is the purpose of a test statistic in hypothesis testing?</li>
<li>Explain the concept of a p-value and its role in hypothesis testing.</li>
<li>What is the difference between a Type I error and a Type II error?</li>
<li>Why is multiple hypothesis testing problematic, even if we control the Type I error rate for each individual test?</li>
<li>What is the family-wise error rate (FWER), and how does it differ from the Type I error rate?</li>
<li>Explain how the Bonferroni correction controls the FWER.</li>
<li>What is the advantage of Holm‚Äôs method over the Bonferroni correction for controlling the FWER?</li>
<li>Define the false discovery rate (FDR). How does it differ from the FWER?</li>
<li>Describe the Benjamini-Hochberg procedure for controlling the FDR.</li>
<li>When might a re-sampling approach be useful for multiple hypothesis testing? Provide an example.</li>
</ol>
</section>
<section id="short-answer-key" class="level2">
<h2 class="anchored" data-anchor-id="short-answer-key">Short Answer Key</h2>
<ol type="1">
<li>A test statistic summarizes the data‚Äôs compatibility with the null hypothesis. Its value helps determine the strength of evidence against the null hypothesis.</li>
<li>The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from the data, assuming the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis.</li>
<li>A Type I error occurs when we reject a true null hypothesis, while a Type II error occurs when we fail to reject a false null hypothesis.</li>
<li>When performing multiple hypothesis tests, the probability of making at least one Type I error increases, even if the individual Type I error rate for each test is controlled. This leads to an inflated overall false positive rate.</li>
<li>The family-wise error rate (FWER) is the probability of making at least one Type I error among all hypotheses tested. Unlike the Type I error rate which focuses on a single test, the FWER considers the probability of making any false rejections across multiple tests.</li>
<li>The Bonferroni correction controls the FWER by dividing the desired overall significance level (alpha) by the number of tests (m). This more stringent significance level for each individual test ensures that the FWER is maintained at or below alpha.</li>
<li>Holm‚Äôs method is a step-down procedure that adjusts the p-value thresholds for each hypothesis sequentially, starting with the smallest p-value. This allows for potentially more rejections compared to the Bonferroni correction, which uses a fixed threshold for all tests, while still controlling the FWER.</li>
<li>The false discovery rate (FDR) is the expected proportion of false rejections (Type I errors) among all rejected hypotheses. It focuses on controlling the rate of false positives among the discoveries made, rather than controlling the probability of making any Type I error like the FWER.</li>
<li>The Benjamini-Hochberg procedure controls the FDR by ranking p-values and finding the largest p-value that satisfies a specific criterion. It then rejects all null hypotheses with p-values less than or equal to this identified p-value.</li>
<li>Re-sampling approaches are useful when the theoretical null distribution of the test statistic is unknown, or when we want to avoid making strong assumptions about the data. For instance, in a two-sample t-test with small sample sizes, where the normality assumption might not hold, we can use permutation testing to approximate the null distribution and calculate p-values.</li>
</ol>
</section>
<section id="essay-questions" class="level2">
<h2 class="anchored" data-anchor-id="essay-questions">Essay Questions</h2>
<ol type="1">
<li>Discuss the trade-off between controlling the FWER and the power of multiple hypothesis tests. How do different correction methods impact this trade-off?</li>
<li>In what situations might controlling the FDR be more appropriate than controlling the FWER? Explain your reasoning with examples.</li>
<li>Compare and contrast the Bonferroni correction and Holm‚Äôs method for controlling the FWER. Discuss their strengths and weaknesses, and provide scenarios where one might be preferred over the other.</li>
<li>Explain the concept of permutation testing and its use in multiple hypothesis testing when theoretical null distributions are unavailable. How does it help in estimating p-values?</li>
<li>Describe a real-world example of multiple hypothesis testing and discuss the implications of choosing different error control methods (FWER, FDR) for the specific problem.</li>
</ol>
</section>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
</section>
<section id="chapter" class="level2">
<h2 class="anchored" data-anchor-id="chapter">Chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch13.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Chapter"><embed src="ch13.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 13: {Multiple} {Testing}},
  date = {2024-09-20},
  url = {https://orenbochman.github.io/notes-islr/posts/ch13-multiple-testing/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 13: Multiple Testing.‚Äù</span>
September 20, 2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch13-multiple-testing/">https://orenbochman.github.io/notes-islr/posts/ch13-multiple-testing/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch13-multiple-testing/</guid>
  <pubDate>Thu, 19 Sep 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Chapter 12: Unsupervised Learning</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-v12.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/kpuQqOzQXfM?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Principal Components
</figcaption>
</figure>
</div><div id="fig-v12.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/O30nHhyBiAs?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Higher order principal components
</figcaption>
</figure>
</div><div id="fig-v12.3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ded_NQqOe7I?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e&amp;t=11" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: k-means clustering
</figcaption>
</figure>
</div><div id="fig-v12.4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/yktzn-Mr2Nw?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Hierarchical Clustering
</figcaption>
</figure>
</div><div id="fig-v12.5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/MYKb5KcI55s?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Matrix Completion
</figcaption>
</figure>
</div><div id="fig-v12.6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/InBhMLEx6sU?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Breast Cancer Example
</figcaption>
</figure>
</div><div id="fig-v12.7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/YCwSrtSoZ9M?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Principal Components
</figcaption>
</figure>
</div><div id="fig-v12.8" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/j6aM9ITqLIY?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: K-means Clustering
</figcaption>
</figure>
</div><div id="fig-v12.9" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v12.9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/dJr7wBUYNgw?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v12.9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Hierarchical Clustering
</figcaption>
</figure>
</div></div>







<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Unsupervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Unsupervised Learning Learning in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Unsupervised Learning Learning in a nutshell"></a></p>
<figcaption>Unsupervised Learning Learning in a nutshell</figcaption>
</figure>
</div>
<ul>
<li><strong>Principal Component Analysis (PCA)</strong> is an unsupervised learning approach that involves only a set of features <img src="https://latex.codecogs.com/png.latex?X_1,%20X_2,%20%5Cldots,%20X_p"> measured on <em>n</em> observations. PCA is not interested in prediction. Instead, the goal is to discover interesting things about the measurements. PCA can be used for data visualization, data pre-processing before supervised learning, and data imputation.</li>
<li><strong>Matrix Completion</strong> is a method for imputing missing data that uses principal components. It can be used if the data is missing at random (i.e., the reason the value is missing is not related to the value itself). Matrix completion is used in recommender systems.</li>
</ul>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<section id="chapter-orientation" class="level2">
<h2 class="anchored" data-anchor-id="chapter-orientation">Chapter Orientation</h2>
<p>This chapter contains some big words, but don‚Äôt worry, we‚Äôll break them down together.</p>
<dl>
<dt>Principal Component Analysis (PCA)</dt>
<dd>
An unsupervised learning approach that involves only a set of features <img src="https://latex.codecogs.com/png.latex?X_1,%20X_2,%20%5Cldots,%20X_p"> measured on <em>n</em> observations. PCA is not interested in prediction. Instead, <mark>the goal is to discover interesting things about the measurements</mark>. PCA can be used for data visualization, data pre-processing before supervised learning, and data imputation.
</dd>
<dt>Principal Components</dt>
<dd>
<mark>Normalized linear combinations of features that have the largest variance</mark> Each principal component is uncorrelated with the others.
</dd>
<dt>Loadings</dt>
<dd>
The elements <img src="https://latex.codecogs.com/png.latex?(%5Cphi_%7B11%7D,%20%5Cphi_%7B21%7D,%20...%20%5Cphi_%7Bp1%7D)"> of the principal component loading vector <img src="https://latex.codecogs.com/png.latex?%5Cphi_%7B11%7D">.
</dd>
<dt>Loading Vector</dt>
<dd>
A vector made up of the loadings <img src="https://latex.codecogs.com/png.latex?%5Cphi_1=(%5Cphi_%7B11%7D,%20%5Cphi_%7B21%7D,%20...%20%5Cphi_%7Bp1%7D)%5ET">. The loading vector defines the direction in feature space along which the data vary the most.
</dd>
<dt>Principal Component Scores</dt>
<dd>
The values that result from projecting the <em>n</em> data points <img src="https://latex.codecogs.com/png.latex?X_1,%20X_2,%20%5Cldots,%20X_p"> onto the direction defined by the loading vector. They can be represented by <em>z</em><sub>11</sub>, ‚Ä¶, z<sub>n1</sub>.
</dd>
<dt>Biplot</dt>
<dd>
<p>A plot that displays both the principal component scores and the loading vectors.</p>
</dd>
<dt>Proportion of Variance Explained (PVE)</dt>
<dd>
<mark>The proportion of variance in a data set that is explained by a principal component.</mark> The PVE can be interpreted as the <em>R</em><sup>2</sup> of the approximation for <em>X</em> given by the first <em>M</em> principal components.
</dd>
<dt>Scree Plot</dt>
<dd>
A plot that depicts the proportion of variance explained by each of the principal components. It can be used to decide how many principal components are needed.
</dd>
<dt>Imputation</dt>
<dd>
Filling in missing values in a data matrix.
</dd>
<dt>Matrix Completion</dt>
<dd>
A method for imputing missing data that uses principal components. It can be used if the data is <strong>missing at random</strong> (i.e., the reason the value is missing is not related to the value itself). <mark>Matrix completion is used in recommender systems.</mark>
</dd>
<dt>Recommender Systems</dt>
<dd>
<p>Systems that use matrix completion to predict a user‚Äôs preferences. They predict a user‚Äôs rating for an item by leveraging their past behavior and the preferences of similar users.</p>
</dd>
<dt>Clustering</dt>
<dd>
<p>A set of techniques used to <strong>find subgroups in a data set</strong> without having an associated response variable <em>Y</em>. Clustering is an unsupervised method because it attempts to discover structure (i.e., distinct clusters) on the basis of a data set.</p>
</dd>
<dt>K-means Clustering</dt>
<dd>
A clustering method that partitions the data into a pre-specified number of clusters (<em>K</em>).
</dd>
<dt>Hierarchical Clustering</dt>
<dd>
A clustering method that does not require knowing the number of clusters in advance. It results in a dendrogram, which allows for simultaneous viewing of clusters obtained for each possible number of clusters (from 1 to <em>n</em>).
</dd>
<dt>Dendrogram</dt>
<dd>
A tree-like visualization of the observations that results from hierarchical clustering. <mark>The height of the cut on the dendrogram serves the same role as the <em>K</em> in K-means clustering</mark>, and determines the number of clusters obtained.
</dd>
<dt>Bottom-up (Agglomerative) Clustering</dt>
<dd>
<p>The most common type of hierarchical clustering. The dendrogram (typically depicted as an upside-down tree) is built by combining clusters starting from the leaves and going up to the trunk.</p>
</dd>
<dt>Linkage</dt>
<dd>
<p>The definition of dissimilarity between two groups of observations. It extends the concept of dissimilarity between a pair of observations to a pair of groups of observations. <strong>The four most common types of linkage are complete, average, single, and centroid.</strong></p>
</dd>
<dt>Inversion</dt>
<dd>
<p>This occurs in centroid linkage, where two clusters are fused at a height below either of the individual clusters in the dendrogram. Inversions can make dendrograms difficult to interpret.</p>
</dd>
</dl>
<section id="chapter-outline" class="level3">
<h3 class="anchored" data-anchor-id="chapter-outline">Chapter Outline</h3>
<p><strong>Unsupervised learning</strong> uses statistical tools to discover information about a set of features <em>X</em><sub>1</sub>, X<sub>2</sub>, ‚Ä¶, X<sub>p</sub> measured on <em>n</em> observations. Unlike supervised learning methods, unsupervised learning methods are not used for prediction because they are not applied to a response variable <em>Y</em>. The goal of unsupervised learning is to explore and understand the relationships and patterns within the features. Unsupervised learning can answer questions like:</p>
<ul>
<li>How can data be visualized informatively?</li>
<li>Can subgroups be found among variables or observations?</li>
</ul>
<p>Some techniques in unsupervised learning include principal components analysis and clustering.</p>
</section>
<section id="applications-of-unsupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-unsupervised-learning">Applications of Unsupervised Learning</h3>
<p>Unsupervised learning is used in various fields, including:</p>
<ul>
<li><strong>Cancer research</strong>: Researchers can use gene expression levels in patients with breast cancer to identify subgroups among patients or genes. This provides a better understanding of the disease.</li>
<li><strong>E-commerce</strong>: Online retailers can identify groups of shoppers with similar buying habits. This enables them to show each shopper items that they are more likely to buy based on the purchase history of similar shoppers.</li>
</ul>
</section>
<section id="principal-components-analysis" class="level3">
<h3 class="anchored" data-anchor-id="principal-components-analysis">Principal Components Analysis</h3>
<p>Principal components analysis (PCA) is a technique used for:</p>
<ul>
<li>Data visualization</li>
<li>Data pre-processing before using supervised techniques</li>
<li>Data imputation (filling in missing values)</li>
</ul>
<section id="what-are-principal-components" class="level4">
<h4 class="anchored" data-anchor-id="what-are-principal-components">What are Principal Components?</h4>
<p>Principal components (PCs) are a way to find a low-dimensional representation of a dataset that captures most of its variation. Each PC is a linear combination of the original features.</p>
<p>Imagine a dataset with <em>n</em> observations and <em>p</em> features. PCA finds a smaller number of dimensions that represent the most variation in the data, allowing the information to be visualized.</p>
</section>
<section id="how-are-principal-components-found" class="level4">
<h4 class="anchored" data-anchor-id="how-are-principal-components-found">How are Principal Components Found?</h4>
<p>PCA finds the linear combination of features, called <em>Z</em><sub>1</sub>, with the largest sample variance. This linear combination is subject to a constraint that ensures the sum of the squares of the coefficients (loadings) equals one.</p>
<p><strong>The first principal component loading vector (a direction in feature space) represents the direction in which the data varies the most.</strong> Projecting the <em>n</em> data points onto this direction gives the principal component scores.</p>
<p>After determining the first PC, subsequent PCs are found in a similar way, but they are constrained to be uncorrelated with the previous PCs. This constraint means that each new PC direction is orthogonal (perpendicular) to the previous directions.</p>
</section>
<section id="proportion-of-variance-explained" class="level4">
<h4 class="anchored" data-anchor-id="proportion-of-variance-explained">Proportion of Variance Explained</h4>
<p>It‚Äôs natural to ask how much information is lost when projecting the observations onto a smaller number of PCs. The proportion of variance explained (PVE) by each PC quantifies this information loss.</p>
<p><strong>The PVE can be interpreted as the <em>R</em></strong><sup>2</sup> of the approximation for the data matrix <em>X</em> using the first <em>M</em> principal components.</p>
</section>
<section id="importance-of-scaling" class="level4">
<h4 class="anchored" data-anchor-id="importance-of-scaling">Importance of Scaling</h4>
<p><strong>Scaling the variables before performing PCA is generally recommended.</strong> If variables are measured in different units or have vastly different variances, scaling ensures that each variable contributes equally to the PCs. This prevents PCs from being dominated by variables with the largest variance.</p>
</section>
<section id="deciding-how-many-principal-components-to-use" class="level4">
<h4 class="anchored" data-anchor-id="deciding-how-many-principal-components-to-use">Deciding How Many Principal Components to Use</h4>
<p>There are several methods to determine the optimal number of PCs:</p>
<ul>
<li><strong>Scree Plot</strong>: A scree plot visualizes the PVE of each component. The ‚Äúelbow‚Äù point in the plot, where the proportion of variance explained drops off, is often chosen as the cutoff for the number of PCs.</li>
<li><strong>Supervised Analysis</strong>: When using PCA for supervised learning, cross-validation can be used to determine the optimal number of PC score vectors to use as features.</li>
<li><strong>Subjective Evaluation</strong>: If using PCA for data exploration, the choice is subjective. It‚Äôs common to examine the first few PCs and continue until no further interesting patterns are found.</li>
</ul>
</section>
<section id="other-uses-for-principal-components" class="level4">
<h4 class="anchored" data-anchor-id="other-uses-for-principal-components">Other Uses for Principal Components</h4>
<p>Principal components can be used as features in various statistical techniques, such as regression, classification, and clustering. Using the PC score vectors instead of the full data matrix can lead to less noisy results. This is because the signal in the data is often concentrated in the first few PCs.</p>
</section>
</section>
<section id="missing-values-and-matrix-completion" class="level3">
<h3 class="anchored" data-anchor-id="missing-values-and-matrix-completion">Missing Values and Matrix Completion</h3>
<p>Missing values in datasets can be a problem for statistical analyses. Matrix completion is a technique that uses PCA to impute missing values by exploiting the correlation between variables. <strong>This is only applicable if the data is missing at random (MAR).</strong> For example, missing a patient‚Äôs weight because the scale‚Äôs battery died is an example of MAR. However, if the weight is missing because the patient was too heavy for the scale, it is not MAR, and matrix completion would not be suitable.</p>
<section id="principal-components-with-missing-values" class="level4">
<h4 class="anchored" data-anchor-id="principal-components-with-missing-values">Principal Components with Missing Values</h4>
<p>Matrix completion seeks to find the principal component score and loading vectors that best approximate the observed data matrix. This involves minimizing the sum of squared differences between the observed data and its approximation based on the PC score and loading vectors.</p>
<p>Once the problem is solved, the missing values can be imputed using the estimated score and loading vectors.</p>
<p><strong>Algorithm 12.1 provides an iterative method for imputing missing values and solving the principal component problem simultaneously.</strong> The algorithm initializes a complete data matrix by replacing missing values with column means. It then iteratively updates the missing values by approximating the matrix using the first M principal components until the error is minimized.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="alg12.1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Matrix Compleation"><img src="https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/alg12.1.png" class="img-fluid figure-img" alt="Matrix Compleation"></a></p>
<figcaption>Matrix Compleation</figcaption>
</figure>
</div>
</section>
</section>
<section id="clustering-methods" class="level3">
<h3 class="anchored" data-anchor-id="clustering-methods">Clustering Methods</h3>
<p>Clustering methods are used to group observations in a dataset into subgroups, or clusters, such that observations within a cluster are similar to each other and different from observations in other clusters. The definition of similarity is often specific to the domain and the data being studied.</p>
<p>For example, a cancer researcher might use clustering to find subgroups of patients with breast cancer based on gene expression measurements. The idea is that different subtypes of breast cancer may exist, and clustering can help discover them.</p>
<section id="types-of-clustering" class="level4">
<h4 class="anchored" data-anchor-id="types-of-clustering">Types of Clustering</h4>
<p>Two common types of clustering methods are:</p>
<ul>
<li><strong>K-means clustering</strong>: This method partitions the data into a pre-specified number of clusters (<em>K</em>).</li>
<li><strong>Hierarchical clustering</strong>: This method does not require specifying the number of clusters beforehand. It produces a tree-like representation of the observations called a dendrogram. The dendrogram shows the clusterings obtained for all possible numbers of clusters, from 1 to <em>n</em>.</li>
</ul>
</section>
<section id="k-means-clustering" class="level4">
<h4 class="anchored" data-anchor-id="k-means-clustering">K-means Clustering</h4>
<p>K-means clustering aims to partition data into <em>K</em> distinct, non-overlapping clusters by minimizing the within-cluster variation.</p>
<p><strong>Algorithm 12.2 outlines the K-means clustering procedure</strong>:</p>
<ol type="1">
<li>Randomly assign each observation to one of the <em>K</em> clusters.</li>
<li>Iterate until the cluster assignments stabilize:
<ul>
<li>Calculate the cluster centroid (the vector of feature means) for each cluster.</li>
<li>Reassign each observation to the cluster with the closest centroid (using Euclidean distance).</li>
</ul></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="alg12.2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="K-means Clustering"><img src="https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/alg12.2.png" class="img-fluid figure-img" alt="K-means Clustering"></a></p>
<figcaption>K-means Clustering</figcaption>
</figure>
</div>
<p><strong>Algorithm 12.2 is guaranteed to find a local optimum for the K-means optimization problem.</strong> The algorithm iteratively improves the clustering by minimizing the sum-of-squared deviations within each cluster.</p>
<p>One challenge with K-means clustering is that the algorithm can converge to different local optima depending on the initial random cluster assignments. To address this, the algorithm can be run multiple times with different initial configurations, and the solution with the lowest objective value is selected.</p>
</section>
<section id="hierarchical-clustering" class="level4">
<h4 class="anchored" data-anchor-id="hierarchical-clustering">Hierarchical Clustering</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="alg12.3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Hierarchical Clustering"><img src="https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/alg12.3.png" class="img-fluid figure-img" alt="Hierarchical Clustering"></a></p>
<figcaption>Hierarchical Clustering</figcaption>
</figure>
</div>
<p>Hierarchical clustering is an alternative approach that doesn‚Äôt require pre-specifying the number of clusters. It creates a dendrogram that visually represents the observations‚Äô hierarchical relationships.</p>
<p><strong>Agglomerative clustering</strong>, a bottom-up approach, is the most common type of hierarchical clustering. <strong>It starts with each observation as its own cluster and iteratively merges the most similar clusters until all observations belong to a single cluster.</strong></p>
<section id="interpreting-a-dendrogram" class="level5">
<h5 class="anchored" data-anchor-id="interpreting-a-dendrogram">Interpreting a Dendrogram</h5>
<p>A dendrogram‚Äôs leaves represent individual observations. As you move up the tree, branches fuse, indicating the merging of similar observations or clusters. <strong>The height of the fusion on the vertical axis represents the dissimilarity between the groups being merged.</strong></p>
<p>It‚Äôs important to note that <strong>the horizontal axis in a dendrogram doesn‚Äôt represent similarity.</strong> The dendrogram can be reordered without changing its meaning, so the proximity of observations on the horizontal axis is not indicative of their similarity.</p>
<p><strong>Clusters can be identified by making a horizontal cut across the dendrogram.</strong> The sets of observations below the cut represent distinct clusters.</p>
</section>
<section id="the-hierarchical-clustering-algorithm" class="level5">
<h5 class="anchored" data-anchor-id="the-hierarchical-clustering-algorithm">The Hierarchical Clustering Algorithm</h5>
<p><strong>Algorithm 12.3 outlines the hierarchical clustering procedure</strong>:</p>
<ol type="1">
<li>Start with <em>n</em> observations as individual clusters and calculate all pairwise dissimilarities.</li>
<li>Iteratively, for <em>i</em> from <em>n</em> down to 2:
<ul>
<li>Find the two clusters with the smallest dissimilarity and merge them.</li>
<li>Update the pairwise dissimilarities between the remaining clusters.</li>
</ul></li>
</ol>
</section>
</section>
<section id="linkage" class="level4">
<h4 class="anchored" data-anchor-id="linkage">Linkage</h4>
<p>Linkage defines the dissimilarity between two groups of observations and significantly influences the hierarchical clustering results. Common linkage types include:</p>
<ul>
<li><strong>Complete linkage</strong>: Uses the maximum dissimilarity between observations in the two clusters.</li>
<li><strong>Single linkage</strong>: Uses the minimum dissimilarity between observations in the two clusters. This method can lead to extended, trailing clusters, where observations are added one by one.</li>
<li><strong>Average linkage</strong>: Uses the average dissimilarity between observations in the two clusters.</li>
<li><strong>Centroid linkage</strong>: Uses the dissimilarity between the centroids of the two clusters. This method can lead to inversions in the dendrogram, which can make interpretation difficult.</li>
</ul>
<p><strong>Average and complete linkage are generally preferred over single linkage because they produce more balanced dendrograms.</strong></p>
</section>
<section id="choice-of-dissimilarity-measure" class="level4">
<h4 class="anchored" data-anchor-id="choice-of-dissimilarity-measure">Choice of Dissimilarity Measure</h4>
<p>The choice of dissimilarity measure can affect the clustering results. Euclidean distance is common, but other measures like correlation-based distance can be more appropriate depending on the context.</p>
<p><strong>Correlation-based distance considers observations similar if their feature values are highly correlated, even if the Euclidean distance is large.</strong> This measure is useful when the shape of the observation profiles is more important than their absolute values.</p>
</section>
</section>
<section id="practical-issues-in-clustering" class="level3">
<h3 class="anchored" data-anchor-id="practical-issues-in-clustering">Practical Issues in Clustering</h3>
<p>There are several practical considerations when using clustering methods:</p>
<ul>
<li><strong>Standardization</strong>: The choice of whether and how to standardize the data can impact the results.</li>
<li><strong>Choice of Parameters</strong>: The dissimilarity measure, linkage type (in hierarchical clustering), the number of clusters (<em>K</em> in K-means clustering), and the dendrogram cut height all need to be chosen carefully.</li>
<li><strong>Cluster Validation</strong>: Evaluating whether the clusters found reflect true subgroups or just noise is important. While methods exist to assess the statistical significance of clusters, there is no single best approach.</li>
<li><strong>Outliers</strong>: Forcing every observation into a cluster can distort the results if outliers exist. Techniques like mixture models can handle outliers more effectively.</li>
<li><strong>Robustness</strong>: Clustering results can be sensitive to data perturbations. It‚Äôs recommended to cluster subsets of the data to understand the robustness of the findings.</li>
</ul>
<p><strong>It‚Äôs crucial to interpret clustering results with caution and consider them as a starting point for further investigation rather than definitive conclusions.</strong> Repeating the analysis with different parameter settings and data subsets can provide a more comprehensive understanding of the data.</p>
<p>The sources provide a lab demonstration of applying PCA and clustering in Python, including code examples for:</p>
<ul>
<li>Performing PCA on the USArrests data.</li>
<li>Implementing matrix completion.</li>
<li>Performing K-means and hierarchical clustering on a simulated dataset.</li>
<li>Analyzing the NCI60 cancer cell line microarray data using PCA and hierarchical clustering.</li>
</ul>
</section>
</section>
<section id="excercises" class="level2">
<h2 class="anchored" data-anchor-id="excercises">Excercises</h2>
<p>Conceptual</p>
<ol type="1">
<li><p>This problem involves the K-means clustering algorithm.</p>
<ol type="a">
<li>Prove (12.18).</li>
<li>On the basis of this identity, argue that the K-means clustering algorithm (Algorithm 12.2) decreases the objective (12.17) at each iteration.</li>
</ol></li>
<li><p>Suppose that we have four observations, for which we compute a dissimilarity matrix, given by<br>
<img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%200&amp;%200.3&amp;%200%20.4&amp;%200%20.7&amp;%20%5C%5C%200.3&amp;%200&amp;%20.5&amp;%200.8%5C%5C%200.4&amp;%200.5&amp;%200&amp;%20.45%5C%5C%200.7%20&amp;0.8&amp;%200.45&amp;0%5Cend%7Bbmatrix%7D%0A"></p>
<p>For instance, the dissimilarity between the first and second observations is 0.3, and the dissimilarity between the second and fourth observations is 0.8.</p>
<ol type="a">
<li>On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observations using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram.</li>
<li>Repeat (a), this time using single linkage clustering.</li>
<li>Suppose that we cut the dendrogram obtained in (a) such that two clusters result. Which observations are in each cluster?</li>
<li>Suppose that we cut the dendrogram obtained in (b) such that two clusters result. Which observations are in each cluster?</li>
<li>It is mentioned in this chapter that at each fusion in the dendrogram, the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram. Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same.</li>
</ol></li>
</ol>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Obs.</th>
<th>X1</th>
<th>X2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0</td>
<td>4</td>
</tr>
<tr class="even">
<td>4</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="odd">
<td>5</td>
<td>6</td>
<td>2</td>
</tr>
<tr class="even">
<td>6</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<ol start="3" type="1">
<li><p>In this problem, you will perform K-means clustering manually, with K = 2 , on a small example with n = 6 observations and p = 2 features. The observations are above.</p>
<ol type="a">
<li>Plot the observations.</li>
<li>Randomly assign a cluster label to each observation. You can use the np.random.choice() function to do this. Report the cluster labels for each observation.</li>
<li>Compute the centroid for each cluster.</li>
<li>Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation.</li>
<li>Repeat (c) and (d) until the answers obtained stop changing.</li>
<li>In your plot from (a), color the observations according to the cluster labels obtained.</li>
</ol></li>
<li><p>Suppose that for a particular data set, we perform hierarchical clustering using single linkage and using complete linkage. We obtain two dendrograms.</p>
<ol type="a">
<li>At a certain point on the single linkage dendrogram, the clusters {1, 2, 3}and {4, 5} fuse. On the complete linkage dendro- gram, the clusters {1, 2, 3}and {4, 5} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?</li>
<li>At a certain point on the single linkage dendrogram, the clusters {5}and {6} fuse. On the complete linkage dendrogram, the clusters {5}and {6}also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?</li>
</ol></li>
<li><p>In words, describe the results that you would expect if you performed K-means clustering of the eight shoppers in Figure 12.16, on the basis of their sock and computer purchases, with K = 2 . Give three answers, one for each of the variable scalings displayed. Explain. 554 12. Unsupervised Learning</p></li>
<li><p>We saw in Section 12.2.2 that the principal component loading and score vectors provide an approximation to a matrix, in the sense of (12.5). Specifically, the principal component score and loading vectors solve the optimization problem given in (12.6). Now, suppose that the M principal component score vectors zim, m = 1, . . . , M , are known. Using (12.6), explain that each of the first M principal component loading vectors œÜjm, m = 1 , . . . , M , can be ob- tained by performing p separate least squares linear regressions. In each regression, the principal component score vectors are the predictors, and one of the features of the data matrix is the response. Applied</p></li>
<li><p>In this chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clus- tering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let rij denote the correlation between the ith and jth observations, then the quantity 1 ‚àírij is proportional to the squared Euclidean distance between the ith and jth observations. On the US Arrests data, show that this proportionality holds. Hint: The Euclidean distance can be calculated using the pairwise_distances() function from the sklearn.metrics module, and pairwise_ distances()correlations can be calculated using the np.corrcoef() function.</p></li>
<li><p>In Section 12.2.3, a formula for calculating PVE was given in Equa- tion 12.10. We also saw that the PVE can be obtained using the explained_variance_ratio_ attribute of a fitted PCA() estimator. On the USArrests data, calculate PVE in two ways:</p>
<ol type="a">
<li>Using the explained_variance_ratio_ output of the fitted PCA() estimator, as was done in Section 12.2.3.</li>
<li>By applying Equation 12.10 directly. The loadings are stored as the components_ attribute of the fitted PCA() estimator. Use those loadings in Equation 12.10 to obtain the PVE. These two approaches should give the same results. Hint: You will only obtain the same results in (a) and (b) if the same data is used in both cases. For instance, if in (a) you performed PCA() using centered and scaled variables, then you must center and scale the variables before applying Equation 12.10 in (b).</li>
</ol></li>
<li><p>Consider the USArrests data. We will now perform hierarchical clus- tering on the states.</p>
<ol type="a">
<li>Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.</li>
<li>Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?</li>
<li>Hierarchically cluster the states using complete linkage and Eu- clidean distance, after scaling the variables to have standard deviation one.</li>
<li>What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.</li>
</ol></li>
<li><p>In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.</p>
<ol type="a">
<li>Generate a simulated data set with 20 observations in each of three classes (i.e.&nbsp;60 observations total), and 50 variables. Hint: There are a number of functions in Python that you can use to generate data. One example is the <code>normal()</code> method of the <code>random()</code> function in <code>numpy</code>; the <code>uniform()</code> method is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes.</li>
<li>Perform PCA on the 60 observations and plot the first two prin- cipal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.</li>
<li>Perform K-means clustering of the observations with K = 3 . How well do the clusters that you obtained in K-means cluster- ing compare to the true class labels? Hint: You can use the <code>pd.crosstab()</code> function in <code>Python</code> to compare the true class labels to the class labels obtained by cluster- ing. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same.</li>
<li>Perform K-means clustering with K = 2 . Describe your results.</li>
<li>Now perform K-means clustering with K = 4 , and describe your results.</li>
<li>Now perform K-means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K-means clustering on the 60 √ó 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.</li>
<li>Using the <code>StandardScaler()</code> estimator, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain.</li>
</ol></li>
<li><p>Write a Python function to perform matrix completion as in Algorithm 12.1, and as outlined in Section 12.5.2. In each iteration, the function should keep track of the relative error, as well as the itera- tion count. Iterations should continue until the relative error is small enough or until some maximum number of iterations is reached (set a default value for this maximum number). Furthermore, there should be an option to print out the progress in each iteration. Test your function on the Boston data. First, standardize the features to have mean zero and standard deviation one using the StandardScaler() function. Run an experiment where you randomly leave out an increasing (and nested) number of observations from 5% to 30%, in steps of 5%. Apply Algorithm 12.1 with M = 1 , 2, . . . , 8. Display the approximation error as a function of the fraction of ob- servations that are missing, and the value of M , averaged over 10 repetitions of the experiment.</p></li>
<li><p>In Section 12.5.2, Algorithm 12.1 was implemented using the <code>svd()</code> function from the <code>np.linalg</code> module. However, given the connection between the <code>svd()</code> function and the <code>PCA()</code> estimator highlighted in the lab, we could have instead implemented the algorithm using PCA(). Write a function to implement Algorithm 12.1 that makes use of <code>PCA()</code> rather than <code>svd()</code>.</p></li>
<li><p>On the book website, <a href="www.statlearning.com">www.statlearning.com</a> there is a gene expression data set (Ch12Ex13.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.</p>
<ol type="a">
<li><p>Load in the data using <code>pd.read_csv()</code>. You will need to select header = None.</p></li>
<li><p>Apply hierarchical clustering to the samples using correlation-based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?</p></li>
<li><p>Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here.</p></li>
</ol></li>
</ol>
</section>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
</section>
<section id="chapter" class="level2">
<h2 class="anchored" data-anchor-id="chapter">Chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch12.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Chapter"><embed src="ch12.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 12: {Unsupervised} {Learning}},
  date = {2024-09-10},
  url = {https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 12: Unsupervised Learning.‚Äù</span>
September 10, 2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/">https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch12-unsupervised-learning/</guid>
  <pubDate>Mon, 09 Sep 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Chapter 9: Support Vector Machines</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch09-svm/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-v1.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v1.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/LvySJGj-88U?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v1.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Opening Remarks
</figcaption>
</figure>
</div><div id="fig-v1.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v1.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/B9s8rpdNxU0?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v1.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples and Framework
</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Support Vector Machines
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Support Vector Machines in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Support Vector Machines in a nutshell"></a></p>
<figcaption>Support Vector Machines in a nutshell</figcaption>
</figure>
</div>
<ul>
<li><strong>Maximal Margin Classifier</strong>: The SVM finds the hyperplane that maximizes the margin between classes.</li>
<li><strong>Support Vector Classifier</strong>: The SVM allows for misclassification by introducing slack variables.</li>
<li><strong>Support Vector Machine</strong>: The SVM extends to nonlinear decision boundaries using kernels.</li>
<li><strong>KKT Conditions</strong>: The Karush-Kuhn-Tucker conditions are necessary for the SVM solution.</li>
<li><strong>Soft Margin</strong>: The SVM can handle noisy data by allowing for misclassification.</li>
<li><strong>Multi-Class SVM</strong>: The SVM can be extended to more than two classes.</li>
</ul>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<section id="slides-and-chapter" class="level2">
<h2 class="anchored" data-anchor-id="slides-and-chapter">Slides and Chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch09.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Chapter"><embed src="ch09.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 9: {Support} {Vector} {Machines}},
  date = {2024-08-10},
  url = {https://orenbochman.github.io/notes-islr/posts/ch09-svm/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 9: Support Vector Machines.‚Äù</span>
August 10, 2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch09-svm/">https://orenbochman.github.io/notes-islr/posts/ch09-svm/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch09-svm/</guid>
  <pubDate>Fri, 09 Aug 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Chapter 6: Linear Model Selection and Regularization</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-v6.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/nsv5rEV3mVI?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Introduction and Best Subset Selection
</figcaption>
</figure>
</div><div id="fig-v6.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ynXq-Gw1xfE?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Stepwise Selection
</figcaption>
</figure>
</div><div id="fig-v6.3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/c5aI9cowjRI?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Backward stepwise selection
</figcaption>
</figure>
</div><div id="fig-v6.4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/48P-oV6cH44?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Estimating test error
</figcaption>
</figure>
</div><div id="fig-v6.5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/mzb5Xs58bb0?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Validation and cross validation
</figcaption>
</figure>
</div><div id="fig-v6.6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/lLlG5xkyqIA?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Shrinkage methods and ridge regression
</figcaption>
</figure>
</div><div id="fig-v6.7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0tfPuddPhEY?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: The Lasso
</figcaption>
</figure>
</div><div id="fig-v6.8" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/KV1Kt6I8rYs?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Tuning parameter selection
</figcaption>
</figure>
</div><div id="fig-v6.9" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/bpto4g5l_go?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Dimension Reduction Methods
</figcaption>
</figure>
</div><div id="fig-v6.10" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v6.10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Uo19ST0IEZI?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v6.10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Principal Components Regression and Partial Least Squares
</figcaption>
</figure>
</div></div>








<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Linear Model Selection and Regularization
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Model Selection and Regularization in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Model Selection and Regularization in a nutshell"></a></p>
<figcaption>Model Selection and Regularization in a nutshell</figcaption>
</figure>
</div>
<p>We cover four main topics in this chapter:</p>
<ol type="1">
<li>Model selection is the process of choosing the best model from a set of candidate models. It involves evaluating trade-offs between model complexity (number of predictors) and model fit (accuracy).</li>
<li>Shrinkage Methods are techniques that constrain or regularize coefficient estimates, effectively shrinking them towards zero. This helps reduce variance and improve prediction accuracy, particularly in high-dimensional datasets. Regularization techniques, such as ridge regression and lasso, help control overfitting by shrinking coefficient estimates towards zero.</li>
<li>Dimension Reduction Methods like principal component regression and partial least squares, reduce the number of predictors used in a model, thus simplifying it and potentially improving performance. Understanding the bias-variance trade-off is essential for selecting the appropriate level of model complexity and regularization.</li>
<li>Cross-validation is a powerful tool for selecting tuning parameters and comparing different models.</li>
</ol>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<section id="terminology-for-model-selection-and-regularization" class="level2">
<h2 class="anchored" data-anchor-id="terminology-for-model-selection-and-regularization">Terminology for Model Selection and Regularization</h2>
<dl>
<dt>Best Subset Selection</dt>
<dd>
A method that evaluates all possible subsets of predictor variables to find the model with the lowest training error.
</dd>
</dl>
<p><a href="alg6.1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/alg6.1.png" class="img-fluid"></a></p>
<dl>
<dt>Forward Stepwise Selection</dt>
<dd>
A method that starts with an empty model and iteratively adds the predictor that most improves the model fit at each step.
</dd>
</dl>
<p><a href="alg6.2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/alg6.2.png" class="img-fluid"></a></p>
<dl>
<dt>Backward Stepwise Selection</dt>
<dd>
A method that starts with a model containing all predictors and iteratively removes the least significant predictor at each step.
</dd>
</dl>
<p><a href="alg6.3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/alg6.3.png" class="img-fluid"></a></p>
<dl>
<dt>Mallow‚Äôs Cp</dt>
<dd>
A statistic used to assess the fit of a model, taking into account both the residual sum of squares and the number of parameters.
</dd>
<dt>Akaike Information Criterion (AIC)</dt>
<dd>
A metric that estimates prediction error and balances model complexity with goodness of fit, favoring simpler models.
</dd>
<dt>Bayesian Information Criterion (BIC)</dt>
<dd>
A similar metric to AIC but with a stronger penalty for model complexity, leading to even simpler models.
</dd>
<dt>Ridge Regression</dt>
<dd>
A shrinkage method that adds an L2 penalty to the least squares objective function, shrinking coefficients towards zero and reducing variance.
</dd>
<dt>Lasso</dt>
<dd>
A shrinkage method that uses an L1 penalty, forcing some coefficients to become exactly zero, leading to variable selection.
</dd>
<dt>Tuning Parameter (Œª)</dt>
<dd>
A parameter that controls the strength of the penalty in shrinkage methods, balancing model fit with coefficient shrinkage.
</dd>
<dt>Cross-Validation</dt>
<dd>
A resampling technique used to estimate the performance of a model on unseen data and to select tuning parameters.
</dd>
<dt>Principal Component Regression (PCR)</dt>
<dd>
A dimension reduction technique that uses principal components (linear combinations of predictors capturing maximum variance) as predictors in regression.
</dd>
<dt>Partial Least Squares (PLS)</dt>
<dd>
A dimension reduction method that constructs components that maximize the covariance between the predictors and the response, aiming for better prediction performance.
</dd>
<dt>Multicollinearity</dt>
<dd>
A situation where predictor variables are highly correlated, which can lead to unstable coefficient estimates in least squares regression.
</dd>
<dt>Bias-Variance Trade-off</dt>
<dd>
The fundamental trade-off between model complexity and prediction accuracy. Simpler models have higher bias but lower variance, while complex models have lower bias but higher variance.
</dd>
<dt>Sparsity</dt>
<dd>
A property of a model where many of the coefficient estimates are exactly zero, indicating that only a subset of the predictors is used in the model.
</dd>
<dt>L1 Norm</dt>
<dd>
The sum of the absolute values of the elements of a vector.
</dd>
<dt>L2 Norm</dt>
<dd>
The square root of the sum of squared elements of a vector.
</dd>
</dl>
</section>
<section id="outline-of-chapter-6" class="level2">
<h2 class="anchored" data-anchor-id="outline-of-chapter-6">Outline of Chapter 6</h2>
</section>
<section id="important-concepts" class="level2">
<h2 class="anchored" data-anchor-id="important-concepts">Important Concepts:</h2>
<ol type="1">
<li><p>Subset Selection Methods:</p>
<ul>
<li><strong>Best Subset Selection</strong>: Evaluates all possible combinations of predictors, choosing the model with the lowest RSS (residual sum of squares) or highest R-squared. Computationally expensive for large datasets.</li>
<li><strong>Forward Stepwise Selection</strong>: Starts with a null model and iteratively adds the predictor that most improves the model fit. Continues until all predictors are included or a stopping criterion is met.</li>
<li><strong>Backward Stepwise Selection</strong>: Starts with the full model and iteratively removes the least useful predictor. Continues until a stopping criterion is met.</li>
<li>Challenges: Selecting the optimal model size requires metrics that estimate test error.</li>
<li>Common Metrics:
<ul>
<li><strong>Mallow‚Äôs <img src="https://latex.codecogs.com/png.latex?C_p"></strong>: Estimates test MSE (mean squared error) based on training RSS and model complexity. Seeks models with low Cp.</li>
</ul>
<blockquote class="blockquote">
<p>‚ÄúMallow‚Äôs Cp is sometimes defined as <img src="https://latex.codecogs.com/png.latex?C'_p%20=%20RSS/%5Chat%7B%5Csigma%7D%5E2%20+%202d%20-%20n">. This is equivalent to the definition given above in the sense that <img src="https://latex.codecogs.com/png.latex?Cp%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Chat%7B%5Csigma%7D%5E2%20(C'_p%20+%20n)">, and so the model with smallest <img src="https://latex.codecogs.com/png.latex?C_p"> also has smallest <img src="https://latex.codecogs.com/png.latex?C'_p">.‚Äù</p>
</blockquote>
<ul>
<li><strong>AIC (Akaike Information Criterion)</strong>: Estimates test MSE using maximum likelihood and model complexity. Favors models with low AIC.</li>
</ul>
<blockquote class="blockquote">
<p>‚ÄúThe AIC criterion is defined for a large class of models fit by maximum likelihood: AIC = ‚àí2 logL+ 2 ¬∑ d where L is the maximized value of the likelihood function for the estimated model.‚Äù</p>
</blockquote>
<ul>
<li><p><strong>BIC (Bayesian Information Criterion)</strong>: Similar to AIC but penalizes model complexity more heavily. Prefers simpler models with low BIC.</p></li>
<li><p><strong>Adjusted R-squared</strong>: Accounts for the number of predictors in the model. Higher adjusted R-squared indicates better fit.</p></li>
</ul></li>
<li>Cross-Validation: Directly estimates test error by splitting data into training and validation sets.</li>
</ul>
<blockquote class="blockquote">
<p>‚ÄúThis procedure has an advantage relative to AIC, BIC, Cp, and adjusted R2, in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model.‚Äù</p>
</blockquote></li>
<li><p>Shrinkage Methods:</p>
<ul>
<li>Ridge Regression: Shrinks coefficient estimates towards zero by adding a penalty term proportional to the sum of squared coefficients. All predictors are included in the final model.</li>
</ul>
<blockquote class="blockquote">
<p>‚ÄúRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúThe penalty <img src="https://latex.codecogs.com/png.latex?%5Clambda%20%5Csum%20%5Cbeta%5E2_j"> in (6.5) will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero unless <img src="https://latex.codecogs.com/png.latex?%5Clambda%20=%20%5Cinfty">.‚Äù</p>
</blockquote>
<ul>
<li>Lasso (Least Absolute Shrinkage and Selection Operator): Shrinks coefficients by adding a penalty term proportional to the sum of absolute coefficient values. Forces some coefficients to be exactly zero, performing variable selection.</li>
</ul>
<blockquote class="blockquote">
<p>‚ÄúThe lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5EL_%5Clambda">#, minimize the quantity‚Ä¶‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúAs with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the %1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter ! is sufficiently large.‚Äù</p>
</blockquote>
<ul>
<li>Benefits: Both methods improve prediction accuracy by reducing variance, but Lasso provides more interpretable models due to variable selection.</li>
<li>Challenges: Selecting the optimal tuning parameter (Œª or s). Cross-validation is widely used for this purpose.</li>
</ul></li>
<li><p>Dimension Reduction Methods:</p>
<ul>
<li><strong>Principal Component Regression</strong> (PCR): Constructs new variables (principal components) as linear combinations of original predictors, capturing most of the variance. Regresses the response on a subset of these components.</li>
</ul>
<blockquote class="blockquote">
<p>‚ÄúPCR identifies linear combinations, or directions, that best represent the predictors <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cldots%20,%20X_p.">‚Äù</p>
</blockquote>
<ul>
<li><strong>Partial Least Squares</strong> (PLS): Similar to PCR but constructs components that are also related to the response variable.</li>
<li>Benefits: Reduce dimensionality, potentially leading to improved prediction accuracy and model interpretability.</li>
<li>Challenges: Choosing the number of components to retain. Cross-validation can be used to determine this.</li>
</ul></li>
</ol>
</section>
<section id="important-considerations-in-high-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="important-considerations-in-high-dimensions">Important Considerations in High Dimensions:</h2>
<p>Traditional methods can struggle with high-dimensional data (p &gt; n). Regularization techniques become crucial for controlling variance and preventing overfitting. Computational efficiency is essential for handling large datasets. Key Takeaways:</p>
<p>Choosing the right method depends on the specific dataset and the goal of the analysis. Understanding the bias-variance trade-off is essential for selecting the appropriate level of model complexity and regularization. Cross-validation is a powerful tool for selecting tuning parameters and comparing different models. Further Research:</p>
<p>Explore different cross-validation strategies and their performance with various model types. Investigate the theoretical properties and limitations of different model selection and regularization methods. Apply these techniques to real-world datasets and evaluate their effectiveness in practice.</p>
</section>
<section id="slides-and-chapter" class="level2">
<h2 class="anchored" data-anchor-id="slides-and-chapter">Slides and Chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch06.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Chapter"><embed src="ch06.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 6: {Linear} {Model} {Selection} and {Regularization}},
  date = {2024-07-20},
  url = {https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 6: Linear Model Selection and
Regularization.‚Äù</span> July 20, 2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/">https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch06-model-selection-and-regularization/</guid>
  <pubDate>Fri, 19 Jul 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Chapter 5: Resampling Methods</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch05-resampling-methods/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-v5.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v5.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/6eWODQJrMKs?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v5.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Cross Validation
</figcaption>
</figure>
</div><div id="fig-v5.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v5.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AMfvd_hLssE?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v5.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: K-fold Cross Validation
</figcaption>
</figure>
</div><div id="fig-v5.3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v5.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/jgoa28FR__Y?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v5.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Cross Validation the wrong and right way
</figcaption>
</figure>
</div><div id="fig-5.4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/h_LweqiIotE?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: K-fold Cross Validation
</figcaption>
</figure>
</div><div id="fig-5.5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AMfvd_hLssE?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: More on the Bootstrap
</figcaption>
</figure>
</div><div id="fig-5.6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/nwD-03ncOZ8?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Lab: Cross Validation
</figcaption>
</figure>
</div><div id="fig-5.7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/sM_Gve1K4II?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Lab: Bootstrap
</figcaption>
</figure>
</div></div>





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Resampling Methods
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Resampling in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Resampling in a nutshell"></a></p>
<figcaption>Resampling in a nutshell</figcaption>
</figure>
</div>
<p>Resampling methods are indispensable tools for data scientists. Cross-validation and the bootstrap empower us to evaluate model performance, estimate uncertainties, and ultimately make more informed decisions based on our statistical learning models.</p>
<blockquote class="blockquote">
<p>‚ÄúThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúLOOCV sometimes useful, but typically doesn‚Äôt shake up the data enough. The estimates from each fold are highly correlated and hence their average can have high variance.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúIn cross-validation, each of the K validation folds is distinct from the other K ‚àí 1 folds used for training: there is no overlap. This is crucial for its success.‚Äù</p>
</blockquote>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<dl>
<dt>Model assessment</dt>
<dd>
the process of evaluating a model‚Äôs performance.
</dd>
<dt>Model selection</dt>
<dd>
the process of selecting the proper level of flexibility for a model.
</dd>
<dt>Test error rate</dt>
<dd>
the average error that results from using a ml method to predict the response on a new observation‚Äî that is, a measurement that was not used in training the method.
</dd>
<dt>Training error rate</dt>
<dd>
the error calculated by applying the ml method to the observations used in its training. This is often dramaticaly less then the test error rate.
</dd>
<dt>Cross-validation</dt>
<dd>
a resampling method used to estimate the test error associated with a given ml method to evaluate its performance or to select the appropriate level of flexibility.
</dd>
<dt>Validation set approach</dt>
<dd>
a method for estimating the test error associated with fitting a particular ml method on a set of observations that involves randomly dividing the available set of observations into two parts, a training set and a validation set.
</dd>
<dt>Validation set error rate</dt>
<dd>
typically assessed using MSE in the case of a quantitative response, provides an estimate of the test error rate.
</dd>
<dt>Leave-one-out cross-validation (LOOCV)</dt>
<dd>
a method that attempts to address the validation set approach‚Äôs drawbacks of high variability and overestimation of the test error rate; involves splitting the set of observations into two parts, but instead of creating two subsets of comparable size, a single observation is used for the validation set, and the remaining observations make up the training set.
</dd>
<dt>Resampling methods</dt>
<dd>
involve repeatedly drawing samples from a training set and refitting a model on each sample to get more information about the fitted model.
</dd>
<dt>Bootstrap</dt>
<dd>
a resampling method used to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.
</dd>
<dt>Hold-out set</dt>
<dd>
the validation set.
</dd>
<dt>k-fold CV</dt>
<dd>
an alternative to LOOCV that involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k - 1 folds.
</dd>
</dl>
<section id="rethinking-test-train-and-validation-sets" class="level2 unnumbered .callout-info">
<h2 class="unnumbered anchored" data-anchor-id="rethinking-test-train-and-validation-sets">rethinking test train and validation sets</h2>
<p>The definition I took away from this course were confused. I want to clarify things abit here. In the ML world, we have do a three way split as follows.</p>
<ol type="1">
<li>training set - used to learn the papemeters.</li>
<li>test set - used to evaluate the model capacity to generalize to new data.</li>
<li>validation set - used to tune the hyperparameters of the model.</li>
</ol>
<p>Once we are finished tuning the hyperparameters we will train the model on all the data using our best hyperparameters and use that for production.</p>
<p>In statitical learning we learned about a number of approaches to do the same thing. The validation set approach, LOOCV, and k-fold CV.</p>
<p>The validation set approach is the simplest but can suffer from if the split underrepresents classes of the data.</p>
<p>LOOCV is computationally expensive for any but the smallest datasets - unless you use a clever trick to estimate it using a single training run. Using such tricks is quite common for work for bayesian hierarchical models. However as the chapter suggests there is also problem of correlation in LOOCV that can lead to high variance in the test error estimate.</p>
<p>k-fold CV is the most common approach and is used in most ML libraries. It is a good balance between computational cost and accuracy. However it is still useful to consider how to pick the k-folds so that they faithfully represent all the classes in the data.</p>
</section>
<section id="outline-of-resampling-methods" class="level2">
<h2 class="anchored" data-anchor-id="outline-of-resampling-methods">Outline of Resampling Methods</h2>
<p>In this chapter we focus on resampling methods, specifically focusing on cross-validation and the bootstrap.</p>
<p>Central Theme: Resampling methods offer robust techniques to estimate the performance of statistical learning methods and quantify uncertainty associated with estimations. This is achieved by repeatedly drawing samples from the training data and refitting models on these samples.</p>
<p>Key Concepts and Methods</p>
<ol type="1">
<li><strong>Validation Set Approach</strong>: This approach involves splitting the dataset into a training set and a validation set. The model is trained on the training set, and its performance (e.g., using MSE for regression problems) is evaluated on the validation set.</li>
</ol>
<ul>
<li>Drawbacks:
<ul>
<li>High variability in test error estimates depending on the random split.</li>
<li>Reduced sample size for training as only the training set is used.</li>
<li>Example: Using the Auto dataset, the sources demonstrate that a quadratic fit performs better than a linear fit for predicting ‚Äòmpg‚Äô based on ‚Äòhorsepower‚Äô.</li>
</ul></li>
</ul>
<ol type="1">
<li><strong>Cross-Validation</strong>: addresses the limitations of the validation set approach. It involves dividing the data into ‚ÄòK‚Äô folds and iteratively using one fold for validation and the rest for training. This provides more stable performance estimates.
<ul>
<li>Leave-One-Out Cross-Validation (LOOCV): A special case where K equals the number of observations. Each observation is held out once for validation.
<ul>
<li>Benefits: Less bias in test error estimates.</li>
<li>Drawbacks: High variance due to high correlation between the training sets.</li>
</ul></li>
<li>k-Fold Cross-Validation: A more general approach where K is typically 5 or 10, balancing bias and variance.
<ul>
<li>Example: Using the Auto dataset, k-fold cross-validation confirms the superiority of the quadratic fit over the linear fit.</li>
</ul></li>
</ul></li>
<li><strong>The Bootstrap</strong>: is used to quantify uncertainty in an estimator. It involves generating multiple ‚Äòbootstrap‚Äô datasets by sampling with replacement from the original data. The statistic of interest is calculated for each bootstrap dataset, creating a distribution from which standard errors and confidence intervals can be derived.
<ul>
<li>Example: Simulating investment returns for assets X and Y, the sources illustrate how the bootstrap can be used to estimate the standard error of the optimal investment allocation (Œ±).</li>
</ul></li>
<li><strong>Bootstrap Applications</strong>:
<ul>
<li><strong>Estimating Standard Errors</strong>: The bootstrap provides an alternative way to calculate standard errors for complex estimators where analytical formulas may not be available.</li>
<li><strong>Confidence Intervals</strong>: Approximate confidence intervals can be derived from the distribution of bootstrap estimates.</li>
<li><strong>Prediction Error Estimation</strong>: While the bootstrap is mainly used for standard error and confidence interval calculations, it‚Äôs not ideal for directly estimating prediction error due to the overlap in training data between bootstrap samples.</li>
</ul></li>
</ol>
</section>
<section id="important-considerations" class="level2">
<h2 class="anchored" data-anchor-id="important-considerations">Important Considerations</h2>
<ul>
<li>Cross-validation for classification: When dealing with classification problems, the metric used for evaluating performance is typically the misclassification rate instead of MSE.</li>
<li>Choosing K: The choice of K in k-fold cross-validation involves a bias-variance trade-off. K = 5 or K = 10 generally provides a good balance.</li>
<li>Pre-validation: This technique is specifically designed for comparing adaptively derived predictors with fixed predictors by creating a ‚Äòfairer‚Äô version of the adaptive predictor that hasn‚Äôt ‚Äòseen‚Äô the response variable.</li>
<li>Bootstrap Sampling: The way bootstrap samples are generated needs careful consideration depending on the data structure. For instance, in time series data, blocks of consecutive observations are sampled instead of individual observations to preserve the temporal correlation.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Resampling methods are indispensable tools for data scientists. Cross-validation and the bootstrap empower us to evaluate model performance, estimate uncertainties, and ultimately make more informed decisions based on our statistical learning models.</p>
</section>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
</section>
<section id="the-chapter" class="level2">
<h2 class="anchored" data-anchor-id="the-chapter">The chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch05.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Chapter"><embed src="ch05.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 5: {Resampling} {Methods}},
  date = {2024-07-10},
  url = {https://orenbochman.github.io/notes-islr/posts/ch05-resampling-methods/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 5: Resampling Methods.‚Äù</span> July
10, 2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch05-resampling-methods/">https://orenbochman.github.io/notes-islr/posts/ch05-resampling-methods/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch05-resampling-methods/</guid>
  <pubDate>Tue, 09 Jul 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Chapter 4: Classification</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch04-classification/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-v4.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ju3J7iRy6xI?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Introduction to Classification Problems
</figcaption>
</figure>
</div><div id="fig-v4.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/kr_Be9NVXOM?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Logistic Regression
</figcaption>
</figure>
</div><div id="fig-v4.3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/1uJVE8bkabc?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Multivariate Logistic Regression
</figcaption>
</figure>
</div><div id="fig-v4.4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/sYDDk6R-be0?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Logistic Regression Case Control Sampling and Multiclass
</figcaption>
</figure>
</div><div id="fig-v4.5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/oJc2r246VoQ?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Discriminant Analysis
</figcaption>
</figure>
</div><div id="fig-v4.6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/14JVlzWHKgk?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Gaussian Discriminant Analysis (One Variable)
</figcaption>
</figure>
</div><div id="fig-v4.7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/aUlTqhDtpnw?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Gaussian Discriminant Analysis (Many Variables)
</figcaption>
</figure>
</div><div id="fig-v4.8" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/n8Nj64FyjSo?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Generalized Linear Models
</figcaption>
</figure>
</div><div id="fig-v4.9" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/giCZkipHEmA?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Quadratic Discriminant Analysis and Naive Bayes
</figcaption>
</figure>
</div><div id="fig-v4.10" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/QEUtuHSipNE?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: R Lab: Logistic Regression
</figcaption>
</figure>
</div><div id="fig-v4.11" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/WXhku-ISml8?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: R Lab: Linear Discriminant Analysis
</figcaption>
</figure>
</div><div id="fig-v4.11" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v4.11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/JRxKBj5ArgU?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v4.11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: R Lab: Nearest Neighbor Classification
</figcaption>
</figure>
</div></div>










<p>The chapter is 65 pages long and covers the following topics:</p>
<ul>
<li>Classification
<ul>
<li>An Overview of Classification</li>
<li>Why Not Linear Regression?</li>
<li>Logistic Regression
<ul>
<li>The Logistic Model</li>
<li>Estimating the Regression Coefficients</li>
<li>Making Predictions</li>
<li>Multiple Logistic Regression</li>
<li>Multinomial Logistic Regression</li>
</ul></li>
<li>Generative Models for Classification
<ul>
<li>Linear Discriminant Analysis</li>
<li>Quadratic Discriminant Analysis</li>
<li>Naive Bayes</li>
</ul></li>
<li>A Comparison of Classification Methods
<ul>
<li>An Analytical Comparison</li>
<li>An Empirical Comparison</li>
</ul></li>
<li>Generalized Linear Models
<ul>
<li>Linear Regression on the Bikeshare Data</li>
<li>Poisson Regression on the Bikeshare Data</li>
<li>Generalized Linear Models in Greater Generality</li>
</ul></li>
<li>Lab: Logistic Regression, LDA, QDA, and KNN</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Statistical Learning in a Nutshell
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Statistical Learning in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Statistical Learning in a nutshell"></a></p>
<figcaption>Statistical Learning in a nutshell</figcaption>
</figure>
</div>
<p>Statistical learning is a set of tools for understanding data and building models that can be used for prediction or inference. The fundamental goal is to learn a function <img src="https://latex.codecogs.com/png.latex?(f)"> that captures the relationship between input variables (predictors) and an output variable (response). This learned function can then be used to make predictions for new observations or for inference i.e.&nbsp;to understand the underlying relationship between the variables.</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<section id="glossary-of-key-terms" class="level2">
<h2 class="anchored" data-anchor-id="glossary-of-key-terms">Glossary of Key Terms</h2>
<dl>
<dt>Classification</dt>
<dd>
The task of predicting a categorical response variable based on a set of predictor variables.
</dd>
<dt>Qualitative Variable</dt>
<dd>
A variable that takes values in an unordered set of categories.
</dd>
<dt>Logistic Regression</dt>
<dd>
A classification method that models the probability of belonging to a category using a logit transformation and a linear combination of predictor variables.
</dd>
<dt>Confounding</dt>
<dd>
A situation where the relationship between a predictor and the response is distorted by the presence of another variable.
</dd>
<dt>Bayes‚Äô Theorem</dt>
<dd>
A mathematical formula that calculates the posterior probability of an event given prior knowledge and new evidence.
</dd>
<dt>Discriminant Analysis</dt>
<dd>
A classification method that uses Bayes‚Äô theorem to classify observations based on the probability of belonging to each class, assuming that the predictor variables follow a certain probability distribution.
</dd>
<dt>LDA (Linear Discriminant Analysis)</dt>
<dd>
A type of discriminant analysis that assumes a common covariance matrix for all classes, resulting in linear decision boundaries.
</dd>
<dt>QDA (Quadratic Discriminant Analysis)</dt>
<dd>
A type of discriminant analysis that allows different covariance matrices for each class, resulting in quadratic decision boundaries.
</dd>
<dt>Naive Bayes</dt>
<dd>
A classification method that assumes conditional independence of predictor variables within each class.
</dd>
<dt>Generalized Linear Model (GLM)</dt>
<dd>
A statistical framework that extends linear regression by allowing for different response variable distributions and a link function that connects the linear predictor to the mean of the response.
</dd>
<dt>Overdispersion</dt>
<dd>
A situation in Poisson regression where the variance of the response variable is larger than the mean.
</dd>
<dt>ROC Curve (Receiver Operating Characteristic Curve)</dt>
<dd>
A graphical plot that illustrates the performance of a binary classifier by plotting the true positive rate against the false positive rate at various threshold settings.
</dd>
<dt>Generative Model</dt>
<dd>
A statistical model that learns the joint probability distribution of the input features and the output variable, allowing for the generation of new data points.
</dd>
<dt>Discriminative Model</dt>
<dd>
A statistical model that directly learns the decision boundary between classes without explicitly modeling the underlying data distribution.
</dd>
<dt>Curse of Dimensionality</dt>
<dd>
The phenomenon where the performance of machine learning algorithms degrades as the number of features increases due to data sparsity and increased computational complexity.
</dd>
</dl>
</section>
<section id="core-concepts" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts">Core Concepts:</h2>
<p>Classification: The task of assigning input data (feature vectors) to specific categories (classes) based on learned patterns. This is contrasted with regression, which predicts continuous outcomes. Logistic Regression: A powerful algorithm for predicting the probability of a binary outcome. It models the log-odds of the outcome as a linear function of the predictors. Discriminant Analysis: A generative approach to classification that assumes data within each class follows a specific distribution, often a Gaussian distribution. Linear Discriminant Analysis (LDA): Assumes the same covariance matrix for all classes, leading to linear decision boundaries. Quadratic Discriminant Analysis (QDA): Allows different covariance matrices for each class, leading to more flexible (quadratic) decision boundaries. Naive Bayes: A simplified generative model that assumes conditional independence among predictors within each class. It‚Äôs computationally efficient and surprisingly effective, even when the independence assumption is violated. Generalized Linear Models (GLMs): A unified framework encompassing linear, logistic, and Poisson regression. They model the relationship between the response variable and predictors through a link function. Model Evaluation: Crucial for assessing the performance of classification models. Key metrics include accuracy, error rates (overall, false positive, false negative), and ROC curves. Important Ideas and Facts:</p>
<p>Classification vs.&nbsp;Regression: While both involve predicting an outcome based on input features, classification deals with discrete categories (e.g., spam/ham, default/no default), whereas regression predicts a continuous value (e.g., house price). Logistic Regression for Probability Prediction: Logistic regression is widely used for binary classification, as it outputs a probability between 0 and 1. The equation relating probability to predictors is: p(X) = e^(Œ≤0 + Œ≤1X) / (1 + e^(Œ≤0 + Œ≤1X)) LDA and QDA: Distributional Assumptions: LDA and QDA rely on the assumption that data within each class follows a multivariate Gaussian distribution. The difference lies in whether they assume a shared covariance matrix (LDA) or class-specific covariance matrices (QDA). Naive Bayes and Conditional Independence: Naive Bayes greatly simplifies the modeling of high-dimensional data by assuming features are independent within each class. While this assumption is often unrealistic, it leads to computational efficiency and can still yield good predictive performance. Generalized Additive Models and Naive Bayes: Naive Bayes can be viewed as a special case of generalized additive models (GAMs). Both allow for non-linear relationships between features and the response variable. Choice of Classification Method: The choice of the best classification method depends on factors like the nature of the data, the number of predictors, the distributional assumptions, and the computational constraints.</p>
</section>
<section id="illustrative-examples" class="level2">
<h2 class="anchored" data-anchor-id="illustrative-examples">Illustrative Examples:</h2>
<p>Default Prediction: The ‚ÄúDefault‚Äù dataset demonstrates logistic regression for predicting credit card default based on balance and student status. This example highlights the importance of interpreting model coefficients and evaluating performance metrics. South African Heart Disease: LDA is applied to predict the risk of myocardial infarction based on various risk factors. This illustrates the use of discriminant analysis for understanding the influence of predictors on a binary outcome. Bikeshare Data: This example explores the use of Poisson regression for modeling count data, showcasing its advantages over linear regression when the variance of the response is related to its mean. ## Key Quotes:</p>
<p>Classification: ‚ÄúGiven a feature vector X and a qualitative response Y taking values in the set C, the classification task is to build a function C(X) that takes as input the feature vector X and predicts its value for Y‚Äù (Ch4_Classification.pdf) Logistic Regression: ‚ÄúThe quantity p(X)/[1-p(X)] is called the odds, and can take on any value between 0 and ‚àû‚Äù (ch04.pdf). LDA Decision Boundary: ‚ÄúIf there are K = 2 classes and œÄ1 = œÄ2 = 0.5, then one can see that the decision boundary is at x = (¬µ1 + ¬µ2) / 2.‚Äù (Ch4_Classification.pdf) Naive Bayes Assumption: ‚ÄúWithin the kth class, the p predictors are independent.‚Äù (ch04.pdf). GLMs: ‚ÄúGeneralized linear models provide a unified framework for dealing with many different response types.‚Äù (Ch4_Classification.pdf) Conclusion:</p>
<p>Classification is a fundamental task in machine learning, with a variety of powerful algorithms at our disposal. Understanding the strengths and weaknesses of each method, along with their underlying assumptions, is essential for selecting the appropriate technique and interpreting the results effectively.</p>
</section>
<section id="slides-and-chapter" class="level2">
<h2 class="anchored" data-anchor-id="slides-and-chapter">Slides and Chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch04.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Chapter"><embed src="ch04.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 4: {Classification}},
  date = {2024-07-01},
  url = {https://orenbochman.github.io/notes-islr/posts/ch04-classification/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 4: Classification.‚Äù</span> July 1,
2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch04-classification/">https://orenbochman.github.io/notes-islr/posts/ch04-classification/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch04-classification/</guid>
  <pubDate>Sun, 30 Jun 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Chapter 3: Linear Regression</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch03-linear-regression/</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>‚ÄúEssentially, all models are wrong, but some are useful‚Äù ‚Äî <a href="https://en.wikipedia.org/wiki/George_E._P._Box">George Box</a></p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Linear Regression <span class="emoji" data-emoji="chart_with_upwards_trend">üìà</span> <span class="emoji" data-emoji="chart_with_downwards_trend">üìâ</span>
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Linear Regression in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Linear Regression in a nutshell"></a></p>
<figcaption>Linear Regression in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>Linear regression is the fundamental statistical technique for modeling the relationship between a predictor variable and a response variable. <mark>In this model we assume a linear relationship between the predictor and the response, represented by a straight line. The goal is to estimate the coefficients of the model that minimize the difference between observed and predicted values.</mark></li>
<li>The model‚Äôs accuracy is assessed <span class="emoji" data-emoji="see_no_evil">üôà</span> <span class="emoji" data-emoji="hear_no_evil">üôâ</span> <span class="emoji" data-emoji="speak_no_evil">üôä</span> via metrics like:
<ul>
<li>the <strong>Residual Standard Error</strong> (RSE) <span class="emoji" data-emoji="broken_heart">üíî</span></li>
<li>the adjusted <img src="https://latex.codecogs.com/png.latex?R%5E2"> <span class="emoji" data-emoji="kiss">üíã</span><br>
</li>
<li>the fantastic <strong>F-statistic</strong>, <span class="emoji" data-emoji="sparkling_heart">üíñ</span> and</li>
<li>the black <span class="emoji" data-emoji="sheep">üêë</span> of statistics - <strong>p-values</strong>. <span class="emoji" data-emoji="scream">üò±</span></li>
</ul></li>
</ul>
<p>The <mark>coefficients are interpreted as the average change in the response for a one-unit increase in the predictor, holding all other variables constant</mark> <span class="emoji" data-emoji="yawning_face">ü•±</span> . Qualitative predictors are incorporated using <span class="emoji" data-emoji="dumpling">ü•ü</span> dummy variables, and interactions between predictors can capture complex relationships.</p>
<p>Model diagnostics help identify potential issues like <strong>non-linearity</strong> <span class="emoji" data-emoji="vampire">üßõ</span>, <strong>heteroscedasticity</strong> <span class="emoji" data-emoji="space_invader">üëæ</span>, <strong>outliers</strong> <span class="emoji" data-emoji="alien">üëΩ</span>, <strong>high leverage points</strong> <span class="emoji" data-emoji="ghost">üëª</span>, and <strong>collinearity</strong> <span class="emoji" data-emoji="zombie">üßü</span>.</p>
<p>Here is a light-hearted deep dive into regression to help you ease into this chapter on linear regression. <span class="emoji" data-emoji="headphones">üéß</span></p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<blockquote class="blockquote">
<p>‚ÄúThe only way to find out what will happen when a complex system is disturbed is to disturb the system, not merely to observe it passively‚Äù ‚Äî <a href="https://en.wikipedia.org/wiki/Frederick_Mosteller">Fred Mosteller</a><sup>1</sup> and <a href="https://en.wikipedia.org/wiki/John_Tukey">John Tukey</a></p>
</blockquote>
<p>The chapter is 64 pages long and covers the following topics:</p>
<ul>
<li>Linear Regression
<ul>
<li>Simple Linear Regression
<ul>
<li>Estimating the Coefficients</li>
<li>Assessing the Accuracy of the Coefficient Estimates</li>
<li>Assessing the Accuracy of the Model</li>
</ul></li>
<li>Multiple Linear Regression
<ul>
<li>Estimating the Regression Coefficients</li>
<li>Some Important Questions</li>
</ul></li>
<li>Other Considerations in the Regression Model
<ul>
<li>Qualitative Predictors</li>
<li>Extensions of the Linear Model</li>
<li>Potential Problems</li>
</ul></li>
<li>The Marketing Plan</li>
<li>Comparison of Linear Regression with K-Nearest Neighbors</li>
</ul></li>
<li>There is a lab:
<ul>
<li><a href="../../posts/ch03-linear-regression/Ch03-linreg-lab.html">Lab: Linear Regression</a></li>
</ul></li>
</ul>
<section id="linear-regression-orientation" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-orientation">Linear Regression Orientation :<span class="emoji" data-emoji="old_key">üóùÔ∏è</span>:</h2>
<p>There are plenty of big words in this chapter, but don‚Äôt worry, we‚Äôll <span class="emoji" data-emoji="muscle">üí™</span> break them down together<span class="emoji" data-emoji="palms_up_together">ü§≤</span>! Here‚Äôs a quick overview of the key concepts covered in this chapter:</p>
<dl>
<dt>Simple Linear Regression</dt>
<dd>
<p>Modeling the relationship between a single input (<em>predictor</em>) variable and an output (<em>response</em>) variable using a straight line.</p>
</dd>
<dt>Multiple Linear Regression</dt>
<dd>
<p>Extending simple linear regression to accommodate multiple <em>predictor</em> variables.</p>
</dd>
<dt>Model Assessment</dt>
<dd>
<p>Evaluating the accuracy and fit of linear regression models using metrics like <strong>RSE</strong>, <img src="https://latex.codecogs.com/png.latex?R%5E2">, <strong>F-statistic</strong>, and <strong>p-values</strong>.</p>
</dd>
<dt>Model Interpretation</dt>
<dd>
<p>Understanding the practical meaning of estimated coefficients and their significance in the context of the data.</p>
</dd>
<dt>Qualitative Predictors</dt>
<dd>
<p>When we want to play with non0numerical predictors, e.g.&nbsp;representing color categories, we can encode them as numerical values or better yet we can use <strong>dummy variables</strong> that track if a category is on <span class="emoji" data-emoji="arrow_forward">‚ñ∂Ô∏è</span> or off <span class="emoji" data-emoji="stop_button">‚èπÔ∏è</span>. Incorporating categorical variables into regression models using <em>dummy variables</em>.</p>
</dd>
<dt>Interactions</dt>
<dd>
<p>Modeling complex relationships by including interaction terms between predictors. If the terms are A and B then the interaction term is <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20A%20%5Ctimes%20B">. where <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is the coefficient representing thr strength of the interaction.</p>
</dd>
<dt>Polynomial Regression</dt>
<dd>
<p>Capturing non-linear relationships using polynomial terms of predictor variables.</p>
</dd>
<dt>Model Diagnostics <span class="emoji" data-emoji="thermometer">üå°Ô∏è</span></dt>
<dd>
<p>Identifying and addressing potential issues <span class="emoji" data-emoji="face_with_thermometer">ü§í</span> like non-linearity, heteroscedasticity, outliers, high leverage points, and collinearity.</p>
</dd>
<dt>Heteroscedasticity <span class="emoji" data-emoji="space_invader">üëæ</span></dt>
<dd>
<p>When the response spreads out more as the predictors increase. You can usually eyeball <span class="emoji" data-emoji="roll_eyes">üôÑ</span> this by looking at the residuals vs.&nbsp;fitted values plot.</p>
</dd>
<dt>Outliers <span class="emoji" data-emoji="alien">üëΩ</span></dt>
<dd>
<p>Observations that don‚Äôt fit the general pattern of the data. If they are mistakes we can toss them out <span class="emoji" data-emoji="wastebasket">üóëÔ∏è</span>. But is they are for real, we call them <strong>high leverage points</strong> <span class="emoji" data-emoji="ghost">üëª</span>.</p>
</dd>
<dt>Collinearity <span class="emoji" data-emoji="zombie">üßü</span></dt>
<dd>
<p>when <em>predictors</em> are highly correlated, leading to unstable coefficient estimates. The <strong>VIF</strong> (Variance Inflation Factor) helps detect collinearity.</p>
</dd>
</dl>
</section>
<section id="simple-linear-regression" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="simple-linear-regression">Simple linear regression</h2>

<div class="no-row-height column-margin column-container"><div id="fig-v3.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v3.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/vCHtY6Me5FI?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v3.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Simple linear regression
</figcaption>
</figure>
</div></div><ul>
<li><p><strong>Model</strong>: The relationship between <em>response</em> <img src="https://latex.codecogs.com/png.latex?(Y)"> and <em>predictor</em> <img src="https://latex.codecogs.com/png.latex?(X)"> is represented as <img src="https://latex.codecogs.com/png.latex?%0AY%20=%20%CE%B2_0%20+%20%CE%B2_1%20X%20+%20%5Cepsilon%0A"></p></li>
<li><p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%CE%B2_0"> is the intercept,</li>
<li><img src="https://latex.codecogs.com/png.latex?%CE%B2_1"> is the slope, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is the error term.</li>
</ul></li>
</ul>
<p>We have two problems:</p>
<ol type="1">
<li><strong>Inference</strong>: how can we find the best values for <img src="https://latex.codecogs.com/png.latex?%CE%B2_0"> and <img src="https://latex.codecogs.com/png.latex?%CE%B2_1"> that minimize the difference between the observed and predicted values of <img src="https://latex.codecogs.com/png.latex?Y">.</li>
<li><strong>Prediction</strong>: given a value of <img src="https://latex.codecogs.com/png.latex?X">, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%CE%B2%7D_0"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%CE%B2%7D_1"> parameters how can we predict the corresponding value of <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D">.</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7By%7D%20=%20%5Chat%7B%CE%B2%7D_0%20+%20%5Chat%7B%CE%B2%7D_1%20x%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> is the predicted value of <img src="https://latex.codecogs.com/png.latex?Y"> for a given value of <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p>and we can define the residuals as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ae_i%20=%20y_i%20-%20%5Chat%7By%7D_i%0A"></p>
<section id="estimating-the-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="estimating-the-coefficients">Estimating the Coefficients</h3>
<ul>
<li><p><strong>Least Squares Estimation</strong>: The coefficients <img src="https://latex.codecogs.com/png.latex?%CE%B2_0"> and <img src="https://latex.codecogs.com/png.latex?%CE%B2_1"> are estimated by minimizing the <strong>Residual Sum of Squares (RSS)</strong>, which quantifies the difference between observed and predicted values.</p></li>
<li><p>The RSS is defined as:</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0ARSS%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(y_i%20-%20%5Chat%7By%7D_i)%5E2%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20e_i%5E2%0A"></p>
<p>least squares estimates are:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%CE%B2%7D_1%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(x_i%20-%20%5Cbar%7Bx%7D)(y_i%20-%20%5Cbar%7By%7D)%7D%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(x_i%20-%20%5Cbar%7Bx%7D)%5E2%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%CE%B2%7D_0%20=%20%5Cbar%7By%7D%20-%20%5Chat%7B%CE%B2%7D_1%20%5Cbar%7Bx%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%7By%7D"> are the sample means of the predictors and response, respectively.</p>
</section>
<section id="assessing-the-accuracy-of-the-coefficient-estimates" class="level3">
<h3 class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</h3>
<ul>
<li><strong>Assessing Accuracy</strong>: The standard errors of the coefficients help construct confidence intervals and perform hypothesis tests to assess the significance of the relationship between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0ASE(%5Chat%7B%CE%B2%7D_1)%5E2%20=%20%5Cfrac%7B%5Csigma%5E2%7D%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(x_i%20-%20%5Cbar%7Bx%7D)%5E2%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0ASE(%5Chat%7B%CE%B2%7D_0)%5E2%20=%20%5Csigma%5E2%20%5Cleft%5B%20%5Cfrac%7B1%7D%7Bn%7D%20+%20%5Cfrac%7B%5Cbar%7Bx%7D%5E2%7D%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(x_i%20-%20%5Cbar%7Bx%7D)%5E2%7D%20%5Cright%5D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20=%20Var%5B%5Cepsilon%5D"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?R%5E2">: This statistic quantifies the proportion of variability in the response explained by the model, indicating how well the model fits the data.</li>
</ul>
<p>We can use these standard errors to compute confidence intervals. A 95% CI is one which will contain the true value of the parameter with a probability of 95%.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Over thinking standard errors <span class="emoji" data-emoji="skull">üíÄ</span><span class="emoji" data-emoji="skull">üíÄ</span><span class="emoji" data-emoji="skull">üíÄ</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>This approach seems strange as we seem to have computed the values precisely using the above estimators. What can possibly change that would give us a different value ?</p>
<p>This is intimated in the slides which mentions repeated sampling?</p>
<p>Where is this repeated sampling -</p>
<p>Some strange Fisherian fantasy that if we repeated the experiment many times we would get different samples each time drawn from some contrafactual distribution that generates the data for each contrafactual version of the experiment.</p>
<p>Getting back to the book we may rephrase this as follows. Since we know the Data generating rule we can simulate data to our hear‚Äôs content. Each time the random number generator spits out different Xs so we get a new data set. For each data set the least square estimator will give us different parameters. Corresponding to a different approximation of the population regression model. (One that has points from all possible experiments). I.e. the point is that we are using <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> from the sample not the population so we will have</p>
<p>the book says</p>
<blockquote class="blockquote">
<p>In other words, in real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved.</p>
</blockquote>
<p>I.e. we don‚Äôt know the Data generating rule. Its likely to be non-linear. But we can compare our approximate rule to the data and estimate the error associated with each parameter. That what the standard error means. (it is the normalized distance the estimator from the data.)</p>
</div>
</div>
</section>
<section id="assessing-the-accuracy-of-the-model" class="level3">
<h3 class="anchored" data-anchor-id="assessing-the-accuracy-of-the-model">Assessing the Accuracy of the Model</h3>
</section>
</section>
<section id="hypothesis-testing-and-confidence-intervals" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="hypothesis-testing-and-confidence-intervals">Hypothesis Testing and Confidence Intervals</h2>

<div class="no-row-height column-margin column-container"><div id="fig-v3.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v3.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/3GiWpRfkSjc?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v3.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Hypothesis Testing and Confidence Intervals
</figcaption>
</figure>
</div><div id="fig-v3.3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v3.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/kr_Be9NVXOM&amp;list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v3.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Multiple Linear Regression
</figcaption>
</figure>
</div></div>
</section>
<section id="some-important-questions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="some-important-questions">Some important questions</h2>

<div class="no-row-height column-margin column-container"><div id="fig-v3.4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v3.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/50sv4UTjE90?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v3.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Some important questions:
</figcaption>
</figure>
</div></div><ol type="1">
<li>Is at least one of the predictors <img src="https://latex.codecogs.com/png.latex?X_1,%20X_2,%20%5Cldots%20,%20X_p"> useful in predicting the response?</li>
<li>Do all the predictors help to explain <img src="https://latex.codecogs.com/png.latex?Y">, or is only a subset of the predictors useful?</li>
<li>How well does the model fit the data?</li>
<li>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</li>
</ol>

<div class="no-row-height column-margin column-container"><div id="fig-v3.5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v3.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/dEBQmiXv9fk?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v3.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Extensions of the Linear Model
</figcaption>
</figure>
</div><div id="fig-v3.5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v3.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/gNZfqHhq_B4?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v3.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Regression in R
</figcaption>
</figure>
</div></div>
</section>
<section id="simple-linear-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="simple-linear-regression-1">Simple Linear Regression:</h2>
</section>
<section id="multiple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression">Multiple Linear Regression:</h2>
<ul>
<li><p><strong>Model</strong>: The model extends to multiple predictors: <img src="https://latex.codecogs.com/png.latex?%0AY%20=%20%CE%B2_0%20+%20%CE%B2_1%20X_1%20+%20%CE%B2_2%20X_2%20+%20%5Cldots%20+%20%CE%B2_p%20X_p%20+%20%5Cepsilon%0A"></p></li>
<li><p><strong>Interpretation</strong>: Each coefficient <img src="https://latex.codecogs.com/png.latex?%CE%B2_j"> represents the average change in <img src="https://latex.codecogs.com/png.latex?Y"> for a one-unit increase in <img src="https://latex.codecogs.com/png.latex?X_j">, holding all other predictors constant.</p></li>
<li><p><strong>F-statistic</strong>: This statistic tests the overall significance of the model, determining whether at least one predictor is useful in predicting the response.</p></li>
<li><p><strong>Variable Selection</strong>: Techniques like <strong>Mallow‚Äôs Cp</strong>, <strong>AIC</strong>, <strong>BIC</strong>, and <strong>adjusted</strong> <img src="https://latex.codecogs.com/png.latex?R%5E2"> are employed to choose the best subset of predictors for the model.</p></li>
</ul>
</section>
<section id="qualitative-predictors" class="level2">
<h2 class="anchored" data-anchor-id="qualitative-predictors">Qualitative Predictors:</h2>
<p>When we have categorical predictors</p>
<ul>
<li><strong>Dummy Variables</strong>: Categorical variables are incorporated by creating dummy variables, which take on values of 0 or 1 to represent different categories.</li>
<li><strong>Baseline Category</strong>: One category is chosen as the baseline, and its coefficient represents the average response for that category. Other dummy variable coefficients represent differences from the baseline.</li>
</ul>
</section>
<section id="interactions" class="level2">
<h2 class="anchored" data-anchor-id="interactions">Interactions:</h2>
<ul>
<li><strong>Interaction Terms</strong>: Including interaction terms like <img src="https://latex.codecogs.com/png.latex?X_1%20X_2"> in the model allows the relationship between one predictor and the response to vary depending on the value of another predictor.</li>
<li><strong>Synergy Effect</strong>: Interactions can capture synergistic effects where the combined impact of two predictors is greater than the sum of their individual impacts.</li>
</ul>
</section>
<section id="polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-regression">Polynomial Regression:</h2>
<ul>
<li><strong>Non-linear Relationships</strong>: Polynomial terms like <img src="https://latex.codecogs.com/png.latex?X%5E2"> are used to model non-linear relationships between predictors and the response.</li>
<li><strong>Overfitting</strong>: Higher-degree polynomials can lead to overfitting, where the model fits the training data too closely but generalizes poorly to new data.</li>
</ul>
</section>
<section id="model-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="model-diagnostics">Model Diagnostics:</h2>
<ul>
<li><strong>Residual Plots</strong>: Visualizing residuals against fitted values helps assess linearity, homoscedasticity, and the presence of outliers.</li>
<li><strong>High Leverage Points</strong>: Observations with extreme predictor values can have a disproportionate impact on the model and should be investigated.</li>
<li><strong>Collinearity</strong>: High correlation among predictors can lead to unstable coefficient estimates and inflated standard errors. <strong>VIF</strong> (Variance Inflation Factor) helps detect collinearity.</li>
</ul>
</section>
<section id="key-quotes" class="level2">
<h2 class="anchored" data-anchor-id="key-quotes">Key Quotes:</h2>
<blockquote class="blockquote">
<p>‚ÄúThe least squares approach chooses <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_0"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_1"> to minimize the RSS.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúWe interpret <img src="https://latex.codecogs.com/png.latex?%CE%B2_j"> as the average effect on <img src="https://latex.codecogs.com/png.latex?Y"> of a one unit increase in <img src="https://latex.codecogs.com/png.latex?X_j"> , holding all other predictors fixed.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúThe woes of (interpreting) regression coefficients: ‚Ä¶ a regression coefficient <img src="https://latex.codecogs.com/png.latex?%CE%B2_j"> estimates the expected change in <img src="https://latex.codecogs.com/png.latex?Y"> per unit change in <img src="https://latex.codecogs.com/png.latex?X_j"> , with all other predictors held fixed. But predictors usually change together!‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúA 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúThere will always be one fewer dummy variable than the number of levels. The level with no dummy variable ‚Äî African American in this example ‚Äî is known as the baseline.‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<p>‚ÄúWhen levels of either TV or radio are low, then the true sales are lower than predicted by the linear model. But when advertising is split between the two media, then the model tends to underestimate sales.‚Äù</p>
</blockquote>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
</section>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
</section>
<section id="chapter" class="level2">
<h2 class="anchored" data-anchor-id="chapter">Chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch03.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Chapter"><embed src="ch03.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>studied attribution problem of the disputed Federalist Papers and authored the amazing ‚ÄúFifty Challenging Problems in Probability‚Äù‚Ü©Ô∏é</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 3: {Linear} {Regression}},
  date = {2024-06-21},
  url = {https://orenbochman.github.io/notes-islr/posts/ch03-linear-regression/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 3: Linear Regression.‚Äù</span> June
21, 2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch03-linear-regression/">https://orenbochman.github.io/notes-islr/posts/ch03-linear-regression/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch03-linear-regression/</guid>
  <pubDate>Thu, 20 Jun 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Chapter 2: Statistical Learning</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-islr/posts/ch02-statistical-learning/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-v2.1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v2.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ox0cKk7h4o0?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v2.1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Introduction to Regression Models
</figcaption>
</figure>
</div><div id="fig-v2.2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v2.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uFwbrdvrAJs?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v2.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Dimensionality and Structured Models
</figcaption>
</figure>
</div><div id="fig-v2.3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v2.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/pvcEQfcO3pk?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v2.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Introduction to Regression Models
</figcaption>
</figure>
</div><div id="fig-v2.4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v2.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/BMJQ3LQ_QKU?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v2.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Classification
</figcaption>
</figure>
</div><div id="fig-v2.5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-v2.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/L03A81OgLlk?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-v2.5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Lab: Introduction to R
</figcaption>
</figure>
</div></div>



<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Statistical Learning
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Statistical Learning in a nutshell"><img src="https://orenbochman.github.io/notes-islr/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Statistical Learning in a nutshell"></a></p>
<figcaption>Statistical Learning in a nutshell</figcaption>
</figure>
</div>
<p>Statistical learning is a set of tools for understanding data and building models that can be used for prediction or inference. The fundamental goal is to learn a function <img src="https://latex.codecogs.com/png.latex?(f)"> that captures the relationship between input variables (predictors) and an output variable (response). This learned function can then be used to make predictions for new observations or for inference i.e.&nbsp;to understand the underlying relationship between the variables.</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<p>The chapter is 53 pages long and covers the following topics:</p>
<ul>
<li>What Is Statistical Learning?</li>
<li>Why Estimate <img src="https://latex.codecogs.com/png.latex?f"> ?</li>
<li>How Do We Estimate <img src="https://latex.codecogs.com/png.latex?f"> ?</li>
<li>The Prediction Accuracy Trade-Off</li>
<li>Model Interpretability</li>
<li>Supervised Versus Unsupervised Learning</li>
<li>Regression Versus Classification Problems</li>
<li>Assessing Model Accuracy
<ul>
<li>Measuring the Quality of Fit</li>
<li>The Bias-Variance Trade-Off</li>
</ul></li>
<li>The Classification Setting</li>
</ul>
<p>There is also a lab which I split into two parts:</p>
<ul>
<li><a href="../../posts/ch02-statistical-learning/Ch02-statlearn-lab.html">Lab: Introduction to Python</a></li>
<li><a href="../../posts/ch02-statistical-learning/Ch02-statlearn-lab-graphics.html">Lab: Graphics</a></li>
</ul>
<section id="key-terms-in-statistical-learning-and-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="key-terms-in-statistical-learning-and-machine-learning">Key Terms in Statistical Learning and Machine Learning</h2>
<dl>
<dt>Statistical Learning</dt>
<dd>
A set of tools for understanding data and building models that can be used for prediction or inference.
</dd>
<dt>Input Variables</dt>
<dd>
Predictors or features, denoted by X, used to predict the output variable.
</dd>
<dt>Output Variable</dt>
<dd>
The response or dependent variable, denoted by Y, being predicted.
</dd>
<dt>Error Term</dt>
<dd>
Represents the random variation in the output variable not explained by the input variables. Denoted by Œµ.
</dd>
<dt>Prediction</dt>
<dd>
Using a statistical learning model to accurately predict the output variable for new observations based on their input values.
</dd>
<dt>Inference</dt>
<dd>
Understanding the relationship between input and output variables, identifying important predictors, and quantifying their effects.
</dd>
<dt>Parametric Methods</dt>
<dd>
Model-based approaches that assume a specific functional form for f and estimate its parameters using training data. Examples include linear regression and logistic regression.
</dd>
<dt>Non-parametric Methods</dt>
<dd>
Flexible approaches that do not pre-specify a functional form for f and estimate it directly from the data. Examples include K-nearest neighbors and splines.
</dd>
<dt>Overfitting</dt>
<dd>
Occurs when a model is too flexible and fits the training data too closely, leading to poor generalization to new data.
</dd>
<dt>Bias</dt>
<dd>
Error resulting from incorrect assumptions about the true functional form of f.
</dd>
<dt>Variance</dt>
<dd>
The amount by which the estimated function fÃÇ changes when trained on different training datasets.
</dd>
<dt>Bias-Variance Trade-off</dt>
<dd>
The balance between model bias and variance, where more flexible models have higher variance but lower bias, and vice versa.
</dd>
<dt>Cross-validation</dt>
<dd>
A technique for evaluating a model‚Äôs performance by splitting the data into multiple folds and using each fold for training and testing, providing a more robust estimate of test error.
</dd>
<dt>Bayes Classifier</dt>
<dd>
A theoretical classifier that assigns each observation to the class with the highest conditional probability given its predictor values, achieving the lowest possible test error rate (Bayes error rate).
</dd>
<dt>K-nearest Neighbors (KNN)</dt>
<dd>
A non-parametric classification method that finds the K nearest neighbors to a test observation in the training data and estimates the conditional probability for each class based on the proportion of neighbors belonging to that class.
</dd>
</dl>
</section>
<section id="chapter-summary" class="level2">
<h2 class="anchored" data-anchor-id="chapter-summary">Chapter summary</h2>
<ol type="1">
<li>What is Statistical Learning?</li>
</ol>
<p><strong>Statistical learning</strong> uses data to learn about relationships between input variables (predictors) and an output variable (response). This relationship can be expressed as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY%20=%20f(X)%20+%20%CE%B5%0A"></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y"> is the response variable.</li>
<li><img src="https://latex.codecogs.com/png.latex?X"> represents the predictors <img src="https://latex.codecogs.com/png.latex?(X_1,%20X_2,...,%20X_p)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?f(X)"> is the unknown function capturing the systematic relationship between X and Y.</li>
<li><img src="https://latex.codecogs.com/png.latex?%CE%B5"> is a random error term with a mean of zero, independent of <img src="https://latex.codecogs.com/png.latex?X">.</li>
</ul>
<p>Statistical learning aims to estimate the function <img src="https://latex.codecogs.com/png.latex?f(X)"> from observed data, enabling:</p>
<ul>
<li><strong>Prediction</strong>: Using the estimated function <img src="https://latex.codecogs.com/png.latex?(f%CC%82)"> to predict <img src="https://latex.codecogs.com/png.latex?Y"> for new values of <img src="https://latex.codecogs.com/png.latex?X">:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%C5%B6%20=%20f%CC%82(X)%0A"></p>
<ul>
<li><p>The accuracy of prediction is influenced by reducible error (due to imperfections in estimating f) and irreducible error (due to the inherent randomness represented by <img src="https://latex.codecogs.com/png.latex?%CE%B5">).</p></li>
<li><p><strong>Inference</strong>: Understanding the relationship between predictors and response. This involves determining which predictors are most important, quantifying the strength of the relationship, and exploring the nature of the association.</p></li>
</ul>
<ol start="2" type="1">
<li>Estimating <img src="https://latex.codecogs.com/png.latex?f(X)"></li>
</ol>
<p>There are two main approaches to estimating the function <img src="https://latex.codecogs.com/png.latex?f(X)">:</p>
<ul>
<li><p><strong>Parametric Methods</strong>: Assume a specific functional form for <img src="https://latex.codecogs.com/png.latex?f(X)">, like a linear model: <img src="https://latex.codecogs.com/png.latex?%0Af(X)%20=%20%CE%B2_0%20+%20%CE%B2_1X_1%20+%20%CE%B2_2X_2%20+%20...%20+%20%CE%B2_pX_p%0A"></p></li>
<li><p>This simplifies the problem to estimating the model‚Äôs parameters (<img src="https://latex.codecogs.com/png.latex?%CE%B2">s). However, parametric methods may be inaccurate if the assumed form doesn‚Äôt match the true relationship.</p></li>
<li><p><strong>Non-Parametric Methods</strong>: Don‚Äôt assume a specific form for <img src="https://latex.codecogs.com/png.latex?f(X)">, allowing for more flexibility. They try to fit the data as closely as possible while maintaining smoothness. Examples include thin-plate splines. These methods require careful tuning to prevent overfitting, where the model fits the training data perfectly but performs poorly on new data.</p></li>
</ul>
<ol start="3" type="1">
<li><p>Assessing Model Accuracy The choice of a suitable statistical learning method and its flexibility depends on the bias-variance trade-off:</p>
<ul>
<li><p><strong>Bias</strong>: Measures how much the estimated function <img src="https://latex.codecogs.com/png.latex?(f%CC%82)"> deviates from the true function <img src="https://latex.codecogs.com/png.latex?(f)"> on average. Inflexible methods tend to have higher bias.</p></li>
<li><p><strong>Variance</strong>: Quantifies the sensitivity of <img src="https://latex.codecogs.com/png.latex?f%CC%82"> to changes in the training data. More flexible methods typically exhibit higher variance.</p></li>
<li><p>The goal is to find the balance between bias and variance that minimizes the expected test error.</p></li>
</ul></li>
<li><p>Supervised vs.&nbsp;Unsupervised Learning</p>
<ul>
<li><p><strong>Supervised Learning</strong>: Uses labeled data, where the response variable <img src="https://latex.codecogs.com/png.latex?(Y)"> is known for each observation. Examples include regression and classification problems.</p></li>
<li><p><strong>Unsupervised Learning</strong>: Deals with unlabeled data, where the response variable is unknown. Techniques like clustering aim to identify groups or patterns in the data.</p></li>
</ul></li>
<li><p>Classification</p></li>
</ol>
<p>Classification problems involve predicting a qualitative (categorical) response variable. One common metric to assess the performance of classifiers is the error rate, which measures the proportion of incorrect predictions.</p>
<ul>
<li><p><strong>Bayes Classifier</strong>: A theoretical classifier that assigns each observation to the most likely class based on its predictors. It achieves the lowest possible error rate (Bayes error rate). However, the Bayes classifier is typically unattainable in practice, as it requires knowing the true conditional distribution of Y given X.</p></li>
<li><p><strong>K-Nearest Neighbors</strong> (KNN): A non-parametric classification method that estimates the conditional probability for each class by considering the closest K training observations to a test point. KNN requires selecting an appropriate value for K, balancing flexibility and overfitting.</p></li>
</ul>
<ol start="6" type="1">
<li>Python for Statistical Learning</li>
</ol>
<p>The chapter introduces basic Python commands and data manipulation techniques using libraries like NumPy and Pandas. It covers:</p>
<ul>
<li>Importing libraries, defining arrays and matrices, computing basic statistics, and generating random data.</li>
<li>Creating various plots (scatter plots, contour plots, heatmaps) using Matplotlib.</li>
<li>Subsetting and indexing data frames, handling missing values, using for loops and lambdas for data manipulation.</li>
</ul>
<p>These tools provide a foundation for applying statistical learning methods in practice.</p>
</section>
<section id="statistical-learning---test-your-understanding" class="level2">
<h2 class="anchored" data-anchor-id="statistical-learning---test-your-understanding">Statistical Learning - Test Your Understanding!</h2>
<ol type="1">
<li>What is the fundamental goal of statistical learning?</li>
<li>Differentiate between the input and output variables in a statistical learning model.</li>
<li>What is the role of the error term in the general form of the statistical learning model?</li>
<li>Explain the difference between prediction and inference in the context of statistical learning.</li>
<li>Describe the two main steps involved in parametric methods for estimating f.</li>
<li>What is a key advantage and a key disadvantage of non-parametric methods compared to parametric methods?</li>
<li>How does the concept of overfitting relate to the choice of flexibility in a statistical learning method?</li>
<li>Explain the bias-variance trade-off and its impact on model selection.</li>
<li>What is the Bayes classifier and why is it considered a gold standard in classification problems?</li>
<li>How does the K-nearest neighbors (KNN) classifier work?</li>
</ol>
<section id="answer-key" class="level3">
<h3 class="anchored" data-anchor-id="answer-key">Answer Key</h3>
<ol type="1">
<li><p>The fundamental goal of statistical learning is to use a set of data to learn a function (f) that captures the relationship between input variables (predictors) and an output variable (response). This learned function can then be used for prediction or inference.</p></li>
<li><p>Input variables, also known as predictors or features, are the variables used to predict the output variable. They are denoted by X. The output variable, also known as the response, is the variable being predicted, denoted by Y.</p></li>
<li><p>The error term (Œµ) represents the random variation in the output variable that cannot be explained by the input variables. It accounts for the inherent uncertainty and noise in the data.</p></li>
<li><p>Prediction focuses on accurately predicting the output variable (Y) for new observations based on their input values (X), often treating the model as a black box. Inference aims to understand the relationship between the input variables and the output variable, focusing on identifying important predictors and their effects on the response.</p></li>
<li><p>First, parametric methods assume a specific functional form for f, such as linear. Second, they estimate the parameters of the assumed function using the training data. For example, in a linear model, the parameters are the coefficients.</p></li>
<li><p>Non-parametric methods are more flexible and can capture complex relationships in the data without pre-specifying a functional form for f.&nbsp;However, this flexibility can lead to overfitting, where the model fits the training data too closely and performs poorly on new data.</p></li>
<li><p>Overfitting occurs when a model is too flexible and captures noise in the training data instead of the underlying signal. This results in high training accuracy but poor generalization to new data. Choosing an appropriate level of flexibility helps avoid overfitting.</p></li>
<li><p>The bias-variance trade-off refers to the balance between model bias (error from wrong assumptions about f) and variance (sensitivity to fluctuations in the training data). More flexible models have higher variance but lower bias, while less flexible models have lower variance but higher bias. Finding the optimal balance minimizes test error.</p></li>
<li><p>The Bayes classifier assigns each observation to the class with the highest conditional probability given its predictor values. It achieves the lowest possible test error rate (Bayes error rate) but is unattainable in practice because the true conditional probability distribution is unknown.</p></li>
<li><p>The KNN classifier finds the K nearest neighbors to a test observation in the training data based on a distance metric. It then estimates the conditional probability for each class based on the proportion of neighbors belonging to that class and classifies the test observation to the class with the highest estimated probability.</p></li>
</ol>
</section>
</section>
<section id="essay-questions" class="level2">
<h2 class="anchored" data-anchor-id="essay-questions">Essay Questions</h2>
<ol type="1">
<li><p>Discuss the differences between supervised and unsupervised learning, providing real-world examples of each.</p>
<p><strong>Supervised learning</strong> is the problem setting in which we have labels on the data. The label are not so much can be a category, a number.<br>
Our model will be shooting an arrow at a know target so it is easier to evaluate the model. This type of problems are broken into regression, classification, survival analysis, and time series analysis, and when learning from exprerience it becomes reinforcement learning.</p>
<p>In unsupervised learning we don‚Äôt have labels. Although we don‚Äôt have a target, this data is much easier to collect as labeling the data requires a significant effort and is often error prone. Typical problems are clustering, dimensionality reduction, and association rule learning. However many problems in natural language processing are unsupervised.</p>
<p>Another point is that the distinction isn‚Äôt so clear today we have <strong>semi-supervised learning</strong> where we have a small amount of labeled data and a large amount of unlabeled data. For example, LLM are pretrained on unsupervised data and then fine-tuned on supervised data.</p></li>
<li><p>Explain the concepts of reducible and irreducible error in statistical learning. How do these errors relate to the concept of the Bayes error rate?</p></li>
<li><p>Compare and contrast parametric and non-parametric methods for estimating f.&nbsp;What are the advantages and disadvantages of each approach? Provide specific examples of methods from each category.</p></li>
<li><p>Describe the concept of cross-validation. Why is it important, and how can it be used to improve model selection and assessment?</p></li>
<li><p>Discuss the challenges and considerations in choosing an appropriate level of flexibility for a statistical learning method. How does the bias-variance trade-off guide this decision? Explain the consequences of choosing a model that is too flexible or not flexible enough.</p></li>
</ol>
</section>
<section id="excercises" class="level2">
<h2 class="anchored" data-anchor-id="excercises">Excercises</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q1: flexible and inflexible
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RjItg-PajpQ?list=PL4OalocKlC1Qt9l6JxBHgRSRbb3ENKFr9" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol type="1">
<li>For each parts, indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.
<ol type="a">
<li>The sample size n is extremely large, and the number of predictors p is small.</li>
<li>The number of predictors p is extremely large, and the numberof observations n is small.</li>
<li>The relationship between the predictors and response is highly non-linear.</li>
<li>The variance of the error terms, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%CF%83%5E2%20=%20Var(%5Cepsilon)">, is extremely high.</li>
</ol></li>
</ol>
<section id="answer" class="level2">
<h2 class="anchored" data-anchor-id="answer">Answer</h2>
<p>Let‚Äôs start by clarifing what we mean by flexible and inflexible.</p>
<ul>
<li>An inflexible model is parametric, and is one that has a sufficently small number of parameters to give a good fit for the data. Henve a multiple regression might be too flexible if there are more predictors than observations I would hazzard that a inflexible model should have an order of magnitude less parameters than observations.</li>
<li>A flexible model is a non-parametric model like a neural network or loess. It should have a higher capacity to fit the data than the inflexible model yet can be trained not to overfit the data.</li>
</ul>
<ol type="a">
<li>In this case I think that we would use a inflexible model. Perhaps because it is simple and is certain to work well. I would consider it as a baseline only consder a more flexible model if I needed to do better.</li>
<li>In this case even a simple model like a multiple regression would underfit the data. I wouls consider a tree based model or pca to reduce the number of predictors to a number that is significantly smaller than the number of observations. Other methods of feature selecetion might also be considered.</li>
<li>The fact that the relationship is non-linear does not neccesirily mean that an inflexible model would not work. If I can find a transformation of the data that makes the relationship linear then I would a multiple regression with the transformed data. If I could not I would use a flexible model like a gaussian process or a neural network.</li>
<li>In this case I would want to avoid a flexible model as it would overfit on the noise of the data. I would prefer a inflexible model like an ANOVA that can model the variance thus reducing the variance of the model. I might also look for some transformation of the data that would reduce the variance of the error terms like a log transformation or a whitening transformation.</li>
</ol>
<p>So in general I would expect an inflexible model to be my first choice and to consider more flexible models only if the inflexible model proved unsatisfactory.</p>
</section>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q2: Classification or Regression
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RjItg-PajpQ?list=PL4OalocKlC1Qt9l6JxBHgRSRbb3ENKFr9" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol start="2" type="1">
<li>Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.</li>
</ol>
<ol type="a">
<li>We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.</li>
<li>We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.</li>
<li>We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.</li>
</ol>
<section id="answer-2" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="answer-2">Answer 2</h2>
<ol type="a">
<li>This is a regression problem with p=3 and n=500 and we are interested in inference.</li>
<li>This is a binary classification problem with p=13 and n=20 and we are interested in prediction.</li>
<li>This is a regression problem with p=3 and n=52 and we are interested in prediction.</li>
</ol>
</section>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q3: Bias-Variance Trade-off
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RjItg-PajpQ?list=PL4OalocKlC1Qt9l6JxBHgRSRbb3ENKFr9" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol start="3" type="1">
<li>We now revisit the bias-variance decomposition.</li>
</ol>
<ol type="a">
<li>Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.</li>
<li>Explain why each of the five curves has the shape displayed in part (a).</li>
</ol>
<section id="answer-3" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="answer-3">Answer 3</h2>
</section>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q4: Real-life Applications
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RjItg-PajpQ?list=PL4OalocKlC1Qt9l6JxBHgRSRbb3ENKFr9" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol start="4" type="1">
<li>You will now think of some real-life applications for statistical learning.</li>
</ol>
<ol type="a">
<li>Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</li>
<li>Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</li>
<li>Describe three real-life applications in which cluster analysis might be useful.</li>
</ol>
<section id="answer-4" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="answer-4">Answer 4</h2>
</section>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q5: Real-life Applications
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RjItg-PajpQ?list=PL4OalocKlC1Qt9l6JxBHgRSRbb3ENKFr9" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol start="5" type="1">
<li>What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?</li>
</ol>
<section id="answer-5" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="answer-5">Answer 5</h2>
</section>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q6: Parametric and a non-parametric
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RjItg-PajpQ?list=PL4OalocKlC1Qt9l6JxBHgRSRbb3ENKFr9" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol start="6" type="1">
<li>Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non- parametric approach)? What are its disadvantages?</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Q7: Real-life Applications
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/RjItg-PajpQ?list=PL4OalocKlC1Qt9l6JxBHgRSRbb3ENKFr9" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>
</section>
<section id="slides" class="level2">
<h2 class="anchored" data-anchor-id="slides">Slides</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Chapter Slides"><embed src="slides.pdf" class="col-page" style="width:5in;height:3.8in"></a></p>
<figcaption>Chapter Slides</figcaption>
</figure>
</div>
</section>
<section id="chapter" class="level2">
<h2 class="anchored" data-anchor-id="chapter">Chapter</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ch02.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Chapter"><embed src="ch02.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Chapter</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Chapter 2: {Statistical} {Learning}},
  date = {2024-06-10},
  url = {https://orenbochman.github.io/notes-islr/posts/ch02-statistical-learning/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúChapter 2: Statistical Learning.‚Äù</span>
June 10, 2024. <a href="https://orenbochman.github.io/notes-islr/posts/ch02-statistical-learning/">https://orenbochman.github.io/notes-islr/posts/ch02-statistical-learning/</a>.
</div></div></section></div> ]]></description>
  <category>notes</category>
  <category>edx</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/notes-islr/posts/ch02-statistical-learning/</guid>
  <pubDate>Sun, 09 Jun 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/notes-islr/images/tiling.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
